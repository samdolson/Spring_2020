---
title: "MATH 392 Problem Set 2"
author: "Sam D. Olson"
output: pdf_document
---


```{r, include=FALSE}
require(tidyverse) # load package
require(tidytext)
options(digits=4) # print 4 decimal places only
set.seed(1999) # set seed for reproducibility
```

#### Time Spent on PS 2: 7 hours

### Exercises from the book

### 7.2.1

$$f \left( \boldsymbol{x} \mid \theta \right) = \theta ^ {n} e ^ { - \theta y}$$

Where $y = \sum \limits_{i=1}^{n} {x_i}$

As Example 7.2.8 provides five observed values, we have $$y=x_1+x_2+...+x_5=16,178$$

If we believe, as the question supposes, that the prior distribution of $\theta$ is the gamma distribution with parameters $1$ and $5000$, we then have a new posterior distribution of $\theta$, $\xi \left( \boldsymbol {x} \mid \theta \right)$ as the gamma distribution with parameters $\alpha_{new} = \alpha_{prior} + 1 = 5 + 1 = 6$ and $\beta_{new} = \beta_{prior} + 5000 = 16178 + 5000 = 21178$ 

When we evaluate the conditional joint p.d.f., we have 
$$f \left( x_n \mid \boldsymbol{x} \right) = \int \limits_{\Omega} \frac {\beta ^ {\alpha}}{\Gamma(\alpha)} \theta ^ {n} e ^ {- \theta y} \theta ^{\alpha-1} e^{-\beta \theta} d \theta$$

We thus have

$$f \left( x_6 \mid \boldsymbol{x} \right) = \int \limits_{0}^{\infty} {(7.518 \cdot 10 ^ {23}) }{\beta ^ {6}} e ^ {- \theta (21178 + x_6)} d \theta$$

For $x_6 > 0$, we may simplify this expression by evaluating this integral, yielding: 

$$f \left( x_6 \mid \boldsymbol{x} \right) =  (7.518 \cdot 10 ^ {23}) \frac {\Gamma(7)} {(21178 + x_6) ^ 7} = \frac {5.413 \cdot 10 ^ {26}}{(21178 + x_6) ^ {7}}$$

We may now compute $Pr (X_6 > 3000 \mid \boldsymbol {x})$ using the above, integrating from $3000$ to $\infty$ with respect to $x_6$. Once we simplify the integral, we may substitute $x_6 = 3000$ and write:

$$Pr (X_6 > 3000 \mid \boldsymbol {x}) =  \int \limits_{3000}^{\infty} \frac {5.413 \cdot 10 ^ {26}}{(21178 + x_6) ^ {7}} dx_6 = \int \limits_{3000}^{\infty} {5.413 \cdot 10 ^ {26} \cdot (21178 + x_6) ^ {-7}} dx_6 = 0.4516$$

### 7.2.2

Note: via Equation 7.2.11: 
$f_n ( x\mid \theta ) = \theta^y(1-\theta)^{n-y}$

For 
$n=8, y = 2$ we then have: 

$f_n ( x\mid \theta\ right) =\theta^2(1-\theta)^6$

Given, 
$\xi(0.1)= 0.7$
and 
$\xi(0.2)=0.3$

$$ \begin {aligned} 
   \xi( 0.1\mid \boldsymbol{x})=Pr(\theta=0.1\mid \boldsymbol{x})
    & = \frac{ \xi(0.1) f_n ( {x}\mid0.1) } {{\xi(0.1) f_n ( {x}\mid0.1) }+ {\xi(0.2) f_n ( {x}\mid0.2) }} \\
    & = \frac{(0.7)(0.1)^2(0.9)^2}{{(0.7)(0.1)^2(0.9)^2}+{(0.3)(0.2)^2(0.8)^2}} \\
    & = 0.5418
\end {aligned}$$

Note: 
$\xi({0.2\mid{x}}) + \xi({0.1\mid{x}}) = 1$

Hence: 
$$ \begin{aligned}
  \xi({0.2\mid{x}}) = 1 -\xi({0.1\mid{x}}) \\
  & = 1 -\frac{(0.7)(0.1)^2(0.9)^2}{{(0.7)(0.1)^2(0.9)^2}+{(0.3)(0.2)^2(0.8)^2}} \\
  & = 1 - 0.5418 \\
  & = 0.4582
\end {aligned}$$

Thus, the posterior pdf of $\theta$ is

$\xi \left( 0.1 \right) = 0.5418$  

$\xi \left( 0.2 \right) = 0.4582$

Intuitively, given the information we are less sure the rate of defective items is $0.1$ and more sure the rate is $0.2$.

### 7.2.3

Let $X$ denote the number of defects on the roll of tape.

For $\lambda$, the p.f. of X, we have, for $x \in \mathbb{Z}^{ \geq {0}}$:

$f \left( x \mid \lambda \right) = \frac { e ^ {-\lambda} \lambda ^ {x}} {x!}$

$$ \begin {aligned} 
  \xi \left( 1.0 \mid x = 3 \right) & = Pr \left( 1.0 \mid x = 3 \right) \\
  & = \frac { \xi \left( 1.0 \right) f \left (3 \mid 1.0 \right) } { \xi \left( 1.0 \right) f \left (3 \mid 1.0 \right) + \xi \left( 1.5 \right) f \left (3 \mid 1.5 \right) }
  \end{aligned}$$ 
  
Note, using the Poisson table in the back of the book, we have:

$f \left( 3 \mid 1.0) = 0.0613 \right)$

$f \left( 3 \mid 1.5) = 0.1255 \right)$

We then note:
$\xi \left( 1.0 \mid 3 \right) + \xi \left( 1.5 \mid 3 \right) = 1$

Hence:
$\xi \left( 1.5 \mid 3 \right) = 1 - \xi \left( 1.0 \mid 3 \right)$

Evaluating yields:
$\xi \left( 1.0 \mid 3 \right) = 0.2456$

$\xi \left( 1.5 \mid 3 \right) = 0.7544$

Thus the posterior p.f. of $\lambda$ is given by:
      $\xi \left( 1.0 \right) = 0.2456$ and 
      $\xi \left( 1.5 \right) = 0.7544$ 

Intuitively, we're now more sure $\lambda = 0.6$ and less sure $\lambda = 0.4$.  

### 7.2.5

Note, the mean and variance of the beta distribution is respectively given by the following:

Mean: $\frac { \alpha} {\alpha + \beta} = \frac {1} {3}$

Variance: $\frac { \alpha \beta} { \left( \alpha + \beta \right) ^{2} \left(\alpha + \beta + 1 \right) }  = \frac {1} {45}$

Given the formula for the mean, we have:

$\alpha + \beta = 3 \alpha$

Hence, 

$\beta = 2 \alpha$ 

It follows, then

$\frac { \alpha} {\alpha + \beta} = \frac {2} {3}$

We may then rewrite the variance as:

$\frac {2} { 9 \left(\alpha + \beta + 1 \right) }  = \frac {1} {45}$

Thus:

$90 = 9 \left( \alpha + \beta + 1 \right)$

And 

$10 = \left( \alpha + \beta + 1 \right)$

$9 = \left( \alpha + \beta  \right)$

As 

$\beta = 2 \alpha$ 

$9 = \left( 3 \alpha \right)$

Hence 

$\alpha = 3$ , $\beta = 6$

As we have established the parameters of the prior p.d.f. of $\theta$, it follows:

$\theta \sim Beta\left( \alpha = 3, \beta = 6 \right)$

Or, alternatively, the prior p.d.f. of $\theta$, is:

$$ \begin{aligned} 
  \xi \left( \theta \right) = \frac{ \Gamma \left( \alpha + \beta \right)} {\Gamma (\alpha) \Gamma (\beta) } \theta ^ {\alpha - 1} (1 - \theta) ^ {\beta - 1 } \\
    & = \frac{ \Gamma \left( \alpha + \beta \right)} {\Gamma (\alpha) \Gamma (\beta) } \theta ^ {\alpha - 1} (1 - \theta) ^ {\beta - 1 } \\
  & = \frac{ \Gamma \left( 9 \right)} {\Gamma (3) \Gamma (6) } \theta ^ {2} (1 - \theta) ^ {5 } 
  \end{aligned}$$

### 7.2.8

Per 7.2.14:

$\xi \left( \theta \mid x_1 \right) \propto f(x_1 \mid \theta) \xi (\theta)$

Extending this to $x_2$ via 7.2.15, we have

$\xi \left( \theta \mid x_1, x_2 \right) \propto f(x_2 \mid \theta) \xi (\theta \mid x_1) = f(x_2 \mid \theta) f(x_1 \mid \theta) \xi (\theta)$

Per 7.2.16, this holds for $x_1, x_2, ..., x_n$, written

$\xi \left( \theta \mid \boldsymbol{x} \right) \propto f(x_n \mid \theta) \xi (\theta \mid x_1, x_2, ..., x_{n-1})$

Via Eq. 7.2.4, we note:

$f_{n-1}(x_1, x_2, ..., x_n) = f(x_1 \mid \theta) f(x_2 \mid \theta) ... f(x_{n-1} \mid \theta)$

Thus, 

$\xi \left( \theta \mid \boldsymbol{x} \right) \propto f(x_n \mid \theta) f(x_{n-1} \mid \theta) ... f(x_{1} \mid \theta)\xi (\theta)$

As $\boldsymbol {x} = x_1, x_2, ..., x_n$

$\xi \left( \theta \mid \boldsymbol{x} \right) \propto f( \boldsymbol {x} \mid \theta)\xi (\theta)$

However, as this is proportional, to make this an equality we must add an appropriate factor. 

Let: 
$\psi = g_n(\boldsymbol{x})^{-1}$, where
$g_n(\boldsymbol{x})= \int_\Omega {f_n(\boldsymbol{x} \mid \theta) \xi (\theta) d\theta}$

Thus, per 7.2.7, we have:

$\xi \left( \theta \mid \boldsymbol{x} \right) = f( \boldsymbol {x} \mid \theta)\xi (\theta) \psi = \frac {f( \boldsymbol {x} \mid \theta)\xi (\theta)} {g_n(\boldsymbol{x})}$

### 7.2.10

Note, $f(x \mid \theta) = 1$ for $\theta - \frac {1} {2} < x < \theta - \frac {1} {2}$ and $0$ otherwise. 

And similarly, $\xi ( \theta ) = \frac {1} {10}$ for $10 < \theta < 20$ and $0$ otherwise.

If we observe $x=12$, we then have

$\xi \left( \theta \mid x=12\right)$ is positive for $11.5 < \theta < 12.5$

Additionally, given X is taken from the uniform distribution, and as $\theta$ is similarly taken from a uniform distribution, the posterior prior distribution is from the uniform distribution. 

Thus, as: 

$\xi \left( \theta \mid x=12\right) \propto f(x \mid \theta) \xi (\theta)$ 

We know the posterior distribution $\xi \left( \theta \mid x=12 \right)$ is uniform for $11.5 < \theta < 12.5$

### 7.3.10

For the random sample taken from the normal distribution with $\theta$ unknown, and standard deviation, $\sigma = 2$, $\sigma^{2} = 4$.

Similarly, as the prior distribution of $\theta$ is a normal distribution with standard deviation $v=1$, $v^{2}=1$. 

Note the variance of the posterior distribution, $\tau^{2}$ which also follows the normal distribution, is given by the following equation: 

$$
\begin{aligned} 
  \tau ^ {2} = \frac { \sigma ^ {2} v ^ {2} } { \sigma ^ {2} + n v^{2} } \\
  & = \frac {4} {4+n} \ 
\end{aligned}
$$

For $\tau = 0.1$, we have:

$\tau ^ {2}  = 0.01 = \frac {4} {4+n}$

Solving for n, we have $n=396$. Thus, we need at least 396 observations to reduce the standard deviation of the posterior distribution of $\theta$, $\tau$ to the value $0.1$.

### 7.3.21

We know the posterior distritubution of $\theta$ with the p.d.f. of the prior distribution of $\theta$ given by $\frac{1} {\theta}$ (for $\theta > 0$) is proportional to:

$$
\begin{aligned}
  \xi \left( \theta \mid \boldsymbol{x} \right) \propto \theta ^ {n} e ^ {- \theta {\sum \limits_{i=1}^{n} {x_i}}} \frac {1} {\theta} \
  & \propto \theta^ {n-1} e ^ {- \theta \bar{x_n} n}
\end{aligned}
$$

Note the posterior distribution of $\theta$ is proportional to the Gamma distribution with parameters $n$ and $n\bar{x_n}$. Hence:

$\xi \left( \theta \mid \boldsymbol{x} \right) \sim Gamma \left( \alpha = n, \beta = n \bar{x_n} \right)$. 

Note, the mean of the Gamma distribution is $fract { \alpha} { \beta}$, thus the mean of the posterior distribution of $\theta$ is:

$$
  \begin{aligned}
    \frac {\alpha} {\beta}
    = \frac {n} {n \bar {x_n} } \
    & = \frac {1} {\bar {x_n} }
  \end{aligned}
$$


### 7.4.5

Per Ex. 5 of Section 7.3, specifically with note of Thm. 7.3.2, 

The posterior distribution of $\theta$ is the Gamma distribution with parameters $\alpha + y$ and $\beta + n$. 

Thus: 

$\alpha = 3 + \sum \limits_{i=1}^{n} {x_i} = 3 + 2 + 2 + 6  +0 +3 = 16$

and 

$\beta = 1 + n = 1 + 5 = 6$

As we now know the parameters of the posterior distribution of $\theta$, we note the Bayes estimate is the mean of the posterior distribution. As such, we nnote the mean of the Gamma distribution is given by: 

$\frac { \alpha} {\beta} = \frac {16} {6} = \frac {8} {3}$.

### 7.4.12

Prior to finding the posterior distribution of $\theta$ for each statistican, note, for $0 < \theta <1$:

Statistican A: $\xi_A \left( \theta \right) = 2\theta \rightarrow$ A's prior distribution for $\theta \sim Beta(2,1)$ 

Statistican B: $\xi_A \left( \theta \right) = 4 \theta ^ {3} \rightarrow$ B's prior distribution for $\theta \sim Beta(4,1)$ 

A. Note Thm. 7.3.4 such that the posterior distribution of $\theta$ is as follows for statisticans A and B respectively. 

(A): $\xi \left( \theta  \mid \boldsymbol{x} \right) \sim Beta \left( \alpha + y, \beta + n - y\right) = Beta \left(2+710, 1+290\right) = Beta \left( 712, 291 \right)$

(B): $\xi \left( \theta  \mid \boldsymbol{x} \right) \sim Beta \left( \alpha + y, \beta + n - y\right) = Beta \left(4+710, 1+290\right) = Beta \left( 714, 291 \right)$

B. Now that we know the parameters of the posterior distribution of $\theta$ for each statistican, note the Bayes estimate for each statistician is equal to the mean of the posterior distribution.

Thus, for the mean of the Beta distribution, $\frac {\alpha} {\alpha + \beta}$, we have:

(A): $\frac {\alpha} {\alpha + \beta} = \frac {712} {712 + 291} = \frac{712}{1003}$

(B): $\frac {\alpha} {\alpha + \beta} = \frac {714} {714 + 291} = \frac{714}{1005}$

C. The difference between the Bayes estimates of statisticans A and B are given by the following equation:

(A): $\frac {2 + y} {2 + y + 1 + 1000 - y} = \frac {2+y} {1003}$

(B): $\frac {4 + y} {4 + y + 1 + 1000 - y} = \frac {4+y} {1005}$

Difference, (A)-(B): $\frac {2+y} {1003} - \frac {4+y} {1005} = \frac { 1003 \left(4 +y\right) - 1005 \left(2+y \right)}{1003(1005)} = \frac { 2002 - 2y} {1003(1005)} = \frac {2 \left(1001-y \right)} {1003(1005)}$

Note, we maximize the difference between the Bayes estimates of A and B when $y=0$, i.e. no one votes in favor of the proposition. For $y=0$, we have:

$\frac {2002}{1003(1005)} = 0.001986 \approx 0.002$

It follows that the Bayes estimates of statisticans A and B could not differ by more than 0.002

### Additional Exercise

For 7.4.12, use the `prop_model()` function in the slides to visualize the prior and posterior distributions of statisticians A and B.

```{r, echo = FALSE, warning = FALSE, message = FALSE}

library(tidyverse)
library(ggridges)
# Function by Rasmus Baath
# This function takes a number of successes and failures coded as a TRUE/FALSE
# or 0/1 vector. This should be given as the data argument.
# The result is a visualization of the how a Beta-Binomial
# model gradualy learns the underlying proportion of successes 
# using this data. The function also returns a sample from the
# posterior distribution that can be further manipulated and inspected.
# The default prior is a Beta(1,1) distribution, but this can be set using the
# prior_prop argument.
# Make sure the packages tidyverse and ggridges are installed, otherwise run:
# install.packages(c("tidyverse", "ggridges"))
# Example usage:
# data <- c(TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE)
# prop_model(data)
prop_model <- function(data = c(), prior_prop = c(1, 1), n_draws = 10000) {
  data <- as.logical(data)
  # data_indices decides what densities to plot between the prior and the posterior
  # For 20 datapoints and less we're plotting all of them.
  data_indices <- round(seq(0, length(data), length.out = min(length(data) + 1, 20)))
  # dens_curves will be a data frame with the x & y coordinates for the 
  # denities to plot where x = proportion_success and y = probability
  proportion_success <- c(0, seq(0, 1, length.out = 100), 1)
  dens_curves <- map_dfr(data_indices, function(i) {
    value <- ifelse(i == 0, "Prior", ifelse(data[i], "Success", "Failure"))
    label <- paste0("n=", i)
    probability <- dbeta(proportion_success,
                         prior_prop[1] + sum(data[seq_len(i)]),
                         prior_prop[2] + sum(!data[seq_len(i)]))
    probability <- probability / max(probability)
    data_frame(value, label, proportion_success, probability)
  })
  # Turning label and value into factors with the right ordering for the plot
  dens_curves$label <- fct_rev(factor(dens_curves$label, levels =  paste0("n=", data_indices )))
  dens_curves$value <- factor(dens_curves$value, levels = c("Prior", "Success", "Failure"))
  p <- ggplot(dens_curves, aes(x = proportion_success, y = label,
                               height = probability, fill = value)) +
    ggridges::geom_density_ridges(stat="identity", color = "white", alpha = 0.8,
                                  panel_scaling = TRUE, size = 1) +
    scale_y_discrete("", expand = c(0.01, 0)) +
    scale_x_continuous("Underlying proportion of success") +
    scale_fill_manual(values = hcl(120 * 2:0 + 15, 100, 65), name = "", drop = FALSE,
                      labels =  c("Prior   ", "Success   ", "Failure   ")) +
    ggtitle(paste0(
      "Binomial model - Data: ", sum(data),  " successes, " , sum(!data), " failures")) +
    theme_light() +
    theme(legend.position = "top")
  print(p)
  # Returning a sample from the posterior distribution that can be further 
  # manipulated and inspected
  posterior_sample <- rbeta(n_draws, prior_prop[1] + sum(data), prior_prop[2] + sum(!data))
  invisible(posterior_sample)
}
```

#### For 7.4.12, use the `prop_model()` function in the slides to visualize the prior and posterior distributions of statisticians A and B.

Note: for this exercise, the order of the 710 'TRUE' values and 290 'FALSE' values is arbitrary. To illustrate this point, the prop_model of Statistican A is illustrated with 'TRUE' values first in the first output, and 'TRUE' values last in the second output. 

--

#### Stat A Pre-Post ('TRUE' first)

```{r a1, eval = FALSE}
trues <- rep(TRUE, times=710)
falses <- rep(FALSE, times=290)
sampledata <- c(trues, falses)

prop_model(sampledata, prior_prop = c(2, 1))
```

--

```{r ref.label = "a1", echo = FALSE, fig.height = 5}
trues <- rep(TRUE, times=710)
falses <- rep(FALSE, times=290)
sampledata <- c(trues, falses)

prop_model(sampledata, prior_prop = c(2, 1))
```

#### Stat A Pre-Post ('TRUE' second)

```{r a2, eval = FALSE}
trues <- rep(TRUE, times=710)
falses <- rep(FALSE, times=290)
sampledata <- c(falses, trues)

prop_model(sampledata, prior_prop = c(2, 1))
```

--

```{r ref.label = "a2", echo = FALSE, fig.height = 5}
trues <- rep(TRUE, times=710)
falses <- rep(FALSE, times=290)
sampledata <- c(falses, trues)

prop_model(sampledata, prior_prop = c(2, 1))
```

#### Stat B Pre-Post

--

```{r b1, eval = FALSE}
trues <- rep(TRUE, times=710)
falses <- rep(FALSE, times=290)
sampledata <- c(trues, falses)

prop_model(sampledata, prior_prop = c(4, 1))
```

--

```{r ref.label = "b1", echo = FALSE, fig.height = 5}
trues <- rep(TRUE, times=710)
falses <- rep(FALSE, times=290)
sampledata <- c(trues, falses)

prop_model(sampledata, prior_prop = c(4, 1))
```

