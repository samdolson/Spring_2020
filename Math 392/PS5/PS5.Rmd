---
title: "MATH 392 Problem Set 5"
author: "Sam D. Olson"
output: 
  pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = FALSE, fig.align = "center", 
                      message = FALSE, warning = FALSE)
library(ggplot2)
library(tidyverse)
library(knitr)
library(here)
```

## Time Spent
8 hours

## Exercise from the book

**8.5 #4**

(Q): Suppose that $X_1, ..., X_n$ form a random sample from the normal distribution with unkown mean $\mu$ and unknown variance $\sigma^2$. How large a random sample must be taken in order that there will be a confidence interval for $\mu$ with confidence coefficient 0.95 and length less than $0.01 \sigma$? 

(A): 

Note: $\frac{\sqrt{n} \left( \bar{X}_n - \mu \right)} {\sigma}$ has a standard normal distribution. 

It then follows:

$Pr \left( -1.96 < \frac{\sqrt{n} \left( \bar{X}_n - \mu \right)} {\sigma} < 1.96 \right) = 0.95$

We isolate the $\mu$ in the above relation, giving us: 

$Pr \left( \bar{X}_n  -1.96 \frac{\sigma}{\sqrt{n}} < \mu  < \bar{X}_n + 1.96 \frac{\sigma}{\sqrt{n}}  \right) = 0.95$

THus, we now have the confidence interval: $\left( \bar{X}_n  -1.96 \frac{\sigma}{\sqrt{n}} ,  \bar{X}_n + 1.96 \frac{\sigma}{\sqrt{n}}  \right)$

As the above interval included $\bar{X}_n$ on both ends of the interval (taking it as a constant for our following observation), we may then note the length of this interval is $1.96 \frac{\sigma}{\sqrt{n}} \cdot  2 = 3.92 \frac{\sigma}{\sqrt{n}}$

If we want a length less than $0.01 \sigma$, then we have: 

$3.92 \frac{\sigma}{\sqrt{n}} < 0.01 \sigma  \rightarrow \frac{3.92}{\sqrt{n}} < 0.01$

Thus we have, for n positive (given the context of the problem):

$3.92 < \sqrt{n} \cdot 0.01 \rightarrow 392 < \sqrt{n} \rightarrow 392^2 < n$

Taking note of the strict inequality (and the fact $392^2 = 153,664$), we need an n sample of size *at least* 153,665 to satisfy the stated condition. 

\newpage

## Case Study: German Tank Problem

Let's pick up the example that we began in class but make the simplifying assumption that we're studying a process where our sample is drawn from the continuous distribution, $X_1, X_2, \ldots, X_n \sim \textrm{Unif}(0, \theta)$, but we're still interested in estimating $\theta$. The MLE and Method of Moments estimators are the same:

$$\begin{aligned}
\hat{\theta}_{MLE} &= \textrm{max}(X_1, X_2, \ldots, X_n) = X_{max} \\
\hat{\theta}_{MOM} &= 2 \bar{X} \\
\end{aligned}$$

***1.*** Calculate the bias of each estimator. If either one is biased, propose an additional estimator that corrects that bias (in the spirit of how $s^2$ is the bias-corrected version of $\hat{\sigma}^2$) . What happens to the bias of these estimators as sample size grows? Plot the relationship between sample size and bias for each estimator (two lines on one plot).


**(A):** 

For $\theta_{MLE}$ we have:

Note, the sampling distribution to $\theta_{MLE}$  is given by:

$$\begin{aligned}
f_{\theta_{MLE}} \left( x \right) =\begin{Bmatrix} 
n \frac{x^{n-1}}{\theta^n} \ for\ 0 \leq x \leq \theta \\
0 \ \ otherwise
\end{Bmatrix}
\end{aligned}$$

Thus, we may evaluate $E_{\theta_{MLE}} (X) = \int \limits_{0}^{\theta} x f_{\theta_{MLE}} dx = \int \limits_{0}^{\theta} x n \frac{x^{n-1}}{\theta^n} dx = \frac{n x ^{n+1}}{(n+1)\theta^n} \Big|_0^\theta = \frac{n}{n+1} \theta$

Thus, we have:

$Bias_{\theta_{MLE}}= E_{\theta_{MLE}} \left( \hat\theta\right) - \theta = \frac{n}{n+1}\theta - \theta = - \frac{1}{n+1}\theta$


**(B):**

For $\theta_{MOM}$ we have:

$E_{\theta_{MOM}}(X) =  \int \limits_{0}^{\theta} x \frac{1}{\theta} dx = \frac{x^2}{2\theta} \Big|_0^\theta  = \frac{\theta}{2}$

$Bias_{\theta_{MOM}}= E_{\theta_{MOM}} \left( \hat\theta\right) - \theta = \frac{2 \theta} {2} - \theta = \theta - \theta = 0$

**(C):** Unbiased MLE estimator

Let $\theta_{adj}$ be the adjusted MLE estimator. We aim to show $\theta_{MLE}$ is bias adjusted. 

Define $\theta_{adj}$ as follows:

$\theta_{adj} = \frac{n+1}{n} max(X_1, ..., X_n) = \frac{n+1}{n} X_{max}$

Note, and referring back to the Bias formulation in (A): 

$Bias_{\theta_{adj}}= E_\theta \left( \theta_{adj} \right) - \theta = \frac{n+1}{n} \frac{n}{n+1} \theta - \theta = \theta - \theta = 0$

Thus, we note $\theta_{adj}$ is the bias adjusted MLE estimator.  

### Plot

```{r echo=TRUE, eval=TRUE, fig.height=3}
theta <- 250
n <- (1:5000)

mle_bias <- rep(NA, 5000) 
mom_bias <- rep(NA, 5000)
for (i in 1:5000){ 
  mle_bias[i] <- (1 / (n[i] + 1)) * theta  
  mom_bias <- 0
}
df <- data.frame(n,mle_bias,mom_bias)

ggplot(df, aes(n)) +                  
  geom_line(aes(y=mle_bias), colour="red") +  
  geom_line(aes(y=mom_bias), colour="green") +
  geom_text(x=200, y=20, label="MLE") +
  geom_text(x=2000, y=5, label="MOM, MLE adjusted") + 
  ggtitle("Bias of MLE, MOM, and adjusted MLE Estimators") +
  labs(y="Bias", x = "n (observations)")
```

The above results indicate the MLE estimate is biased (but unbiased asymptotically as n increases) and the MOM estimate is unbiased.


***2.*** Calculate the variance of each estimator (including any new bias-corrected ones). What happens as sample size grows? Create an analogous plot to the one above.

**(A):** 

For $\theta_{MLE}$ we have:

$E_{\theta_{MLE}} (X^2) = \int \limits_{0}^{\theta} x^2 f_{\theta_{MLE}} dx = \int \limits_{0}^{\theta} x^2 n \frac{x^{n-1}}{\theta^n} dx = \frac{n x ^{n+2}}{(n+2)\theta^n} \Big|_0^\theta = \frac{n}{n+2} \theta^2$

$Var_{\theta_{MLE}}= E_{\theta_{MLE}} \left( {\hat\theta} ^ {2} \right) - \left(E_{\theta_{MLE}} \left( {\hat\theta}\right)\right) ^ {2}$

$=\frac{n}{n+2}\theta^2 - \left( \frac{n}{n+1} \theta \right)^2 = \frac{n}{ \left( n+1 \right) ^ 2 \left(n+2 \right)} \theta^2$


**(B):**

For $\theta_{MOM}$ we have:

$E_{\theta_{MOM}}(X^2) =  \int \limits_{0}^{\theta} x^2 \frac{1}{\theta} dx = \frac{x^3}{3\theta} \Big|_0^\theta  = \frac{\theta}{3}$

Thus, we have:

$Var_{\theta_{MOM}}= E_{\theta_{MOM}} \left( {\hat\theta} ^ {2} \right) - \left(E_{\theta_{MOM}} \left( {\hat\theta}\right)\right) ^ {2}$

$= \frac {\theta^2}{3} - \left( \frac{\theta}{2}\right)^2 = \frac{\theta^2}{3} - \frac{\theta^2}{4} = \frac{\theta^2}{12}$

Thus:

$V_\theta \left( \hat{\theta}_n \right) = \frac{4}{n^2} n V_\theta \left( X\right) = \frac{4}{n^2} n \frac{\theta^2}{12} = \frac{\theta^2} {3n}$

**(C):**

For $\theta_{adj}$ we have:

$Var_\theta \left(\theta_{adj} \right) = Var_\theta \left(\frac{n+1}{n} \theta_{MLE} \right) = \left( \frac{n+1}{n} \right)^2 Var_\theta (\theta_{MLE})$

As we again solved the variance of the initial MLE estimator above, we have:

$Var_\theta \left(\theta_{adj} \right) = \left( \frac{n+1}{n} \right)^2 \cdot \frac{n}{ \left( n+1 \right) ^2 \left(n+2 \right)} \theta^2  = \frac{1}{n \left( n+2 \right)} \theta^2$

### Plot & Commentary

```{r echo=TRUE, eval=TRUE, fig.heigh=3}
theta <- 250
n <- (1:50)

mle_var <- rep(NA, 50) 
mom_var <- rep(NA, 50)
mle_adj_var <- rep(NA, 50)
for (i in 1:50){ 
  mle_var[i] <- ((theta ^ 2) * (n[i]) )/ (((n[i] + 1) ^ 2) * (n[i] + 2))
  mom_var[i] <- (theta ^ 2) / (3 * n[i])
  mle_adj_var[i] <- (theta ^ 2) / ((n[i] * (n[i] +2)))
}

df <- data.frame(n,mle_var,mom_var, mle_adj_var)

ggplot(df, aes(n)) +                  
  geom_line(aes(y=mle_var), colour="red") +  
  geom_line(aes(y=mom_var), colour="green") +
  geom_line(aes(y=mle_adj_var), colour="blue") +
  geom_text(x=2.5, y=1000, label="MLE") +
  geom_text(x=5, y=20000, label="MOM") +
  geom_text(x=10, y=5000, label="MLE adj.") +
  ggtitle("Variance of MLE, MOM, and adjusted MLE Estimators") +
  labs(y="variance", x = "n (observations)")
```

As the sample size grows, the variance of all three estimators converge to 0, though the mle estimator converges fastest (relatively), followed by the adjusted mle estimator. 


***3.*** Combine the notions of bias and variance into a third plot that shows how the Mean Squared Error changes as a function of sample size. Based on this plot, which estimator would you use and why?

Note, the formula for the M.S.E. is given as follows:

$M.S.E. \left( \hat{\theta} \right) = Var (\hat{\theta}) + Bias(\hat{\theta})^2$

Thus, using the above relations we have: 


**(A):** 

$M.S.E. \left( \theta_{MLE} \right) = Var (\theta_{MLE}) + Bias(\theta_{MLE})^2 = \frac{n}{ \left( n+1 \right) \left(n+2 \right)} \theta^2 + \left( - \frac{1}{n+1}\theta \right)^2 = \frac{n \theta^2}{ \left( n+1 \right) \left(n+2 \right)} + \frac{\theta^2}{ \left( n+1\right) ^2} = \frac{2 \theta^2 } {(n+1)(n+2)}$


**(B):** 

Similarly, for the MOM we have:

$M.S.E. \left( \theta_{MOM} \right) = Var (\theta_{MOM}) + Bias(\theta_{MOM})^2 = \frac{\theta^2} {3n} + 0^2 = \frac{\theta^2} {3n}$


**(C):** 

For the adjusted MLE estimator, we first note it has no bias, like the MOM estimator in (B). Thus, it follows:

$M.S.E. \left( \theta_{adj} \right) = Var (\theta_{adj}) + Bias(\theta_{adj})^2 = \frac{1}{n \left( n+2 \right)} \theta^2 + 0^2 = \frac{1}{n \left( n+2 \right)} \theta^2$

### Plot & Commentary

```{r echo=TRUE, eval=TRUE, fig.heigh=3}
theta <- 250
n <- (1:50)

mle_mse <- rep(NA, 50) 
mom_mse <- rep(NA, 50)
mle_adj_mse <- rep(NA, 50)
for (i in 1:50){ 
  mle_mse[i] <- ((theta ^ 2) * 2 )/ ((n[i] + 1) * (n[i] + 2))
  mom_mse[i] <- (theta ^ 2) / (3 * n[i])
  mle_adj_mse[i] <- (theta ^ 2) / ((n[i] * (n[i] +2)))
}

df <- data.frame(n,mle_mse,mom_mse, mle_adj_mse)

ggplot(df, aes(n)) +                  
  geom_line(aes(y=mle_mse), colour="red") +  
  geom_line(aes(y=mom_mse), colour="green") +
  geom_line(aes(y=mle_adj_mse), colour="blue") +
  geom_text(x=3.5, y=500, label="MLE adj.") +
  geom_text(x=20, y=2500, label="MOM") +
  geom_text(x=8, y=3000, label="MLE") + 
  ggtitle("MSE of MLE, MOM, and adjusted MLE Estimators") +
  labs(y="MSE", x = "n (observations)")
```

Note, as shown in the above plot, the adjusted MLE estimate converges 'fastest' to 0. Thus, we would anticipate that for a smaller number of observations, the adjusted MLE will produce estimates with lower MSE than the unadjusted MLE and the MOM estimators. 

Though, if I'm being cheeky, for $n=1$, both the adjusted MLE and the MOM have the same MSE. (In that instance...flip a coin? Or in all seriousness, get one more observation and use the adjusted MLE.)


***4.*** Using the method that we saw in class based on Markov's Inequality, assess whether each of these estimators is consistent.

We can demonstrate consistency for each estimator by showing the following two relations:

(1): $\lim_{n \rightarrow \infty} Var\left( \hat{\theta}_n \right) = 0$

(2): $\lim_{n \rightarrow \infty} Bias \left( \hat{\theta}_n \right) = 0$

Thus, if both (1) and (2) hold for an estimator $\hat{\theta}_n$, we may take advantage of Markov's Inequality, noting that for any constant c ($c>0$):

$\lim_{n \rightarrow \infty} Pr \left( \mid \hat{\theta}_n - \theta \mid > c \right) = 0$ and we say $\hat{\theta}_n$ is a consistent estimator. 

**(A):**

$Bias_{\theta_{MLE}}= E_{\theta_{MLE}} \left( \hat\theta\right) - \theta = \frac{n}{n+1}\theta - \theta = - \frac{1}{n+1}\theta$

Note, as $n \rightarrow \infty$, $Bias_{\theta_{MLE}} \rightarrow 0$. Thus, we say ${\theta_{MLE}}$ is assymptotically unbiased. We have thus satisfied condition (1). 

For condition (2), we note the variance of $\theta_{MLE}$:

$V_\theta \left( \hat{\theta}_{MLE} \right) = \frac{n}{ \left( n+1 \right) ^ 2 \left(n+2 \right)} \theta^2$

Taking the limit as $n \rightarrow \infty$ shows $Var_{\theta_{MLE}} \rightarrow 0$ as well. Thus we have satisfied condition (2). 

As conditions (1) and (2) are satisfied, we thus note, for any constant c ($c>0$):

$\lim_{n \rightarrow \infty} Pr \left( \mid \hat{\theta}_{MLE} - \theta \mid > c \right) = 0$ and we say $\hat{\theta}_{MLE}$ is a consistent estimator. 

**(B):**

Note, for the estimator $\theta_{MOM}$, the estimator is unbiased, thus we satisfy condition (1). 

For condition (2) we note the variance of $\theta_{MOM}$:

$V_\theta \left( \hat{\theta}_{MOM} \right) = \frac{\theta^2} {3n}$

For $n \rightarrow \infty$, $Var \left( \hat{\theta}_{MOM} \right) \rightarrow 0$. 

As conditions (1) and (2) are satisfied, we may then note, for any constant c ($c>0$):

$\lim_{n \rightarrow \infty} Pr \left( \mid \hat{\theta}_{MOM} - \theta \mid > c \right) = 0$ and we say $\hat{\theta}_{MOM}$ is a consistent estimator. 

**(C):**

Note, for the estimator $\theta_{adj}$, the estimator is unbiased, thus we satisfy condition (1).

Thus, to satisfy condition (2) we note the variance of $\theta_{adj}$: 

$Var_\theta \left(\theta_{adj} \right) = \frac{1}{n \left( n+2 \right)} \theta^2$

We then note, as $n \rightarrow \infty$, $Var_\theta \left(\theta_{adj} \right) \rightarrow 0$. Thus we have satisfied condition (2). 

As conditions (1) and (2) have been satisfied, then we can say, for any constant c ($c>0$):

$\lim_{n \rightarrow \infty} Pr \left( \mid \hat{\theta}_{adj} - \theta \mid > c \right) = 0$ and we say $\hat{\theta}_n$ is a consistent estimator. 

***5.*** What is the sampling distribution of each statistic? For the MOM, consider both the Irwin-Hall distribution and a sensible approximation based on the Central Limit Theorem.

(A): 

Note, the cdf of $\hat{\theta}_{MLE}$ is given as follows:

$$\begin{aligned}
f\left( u \right)  = \begin{Bmatrix} 
0 \ for\ u \leq \theta \\
\left(\frac{u}{\theta}\right)^n \ for\ 0 < u < \theta \\
1 \ for\ u \geq \theta
\end{Bmatrix}
\end{aligned}$$

Thus, to determine the pdf of $\hat{\theta}_{MLE}}{2 \theta}$, we derive the above equation with respect to u. Thus gives us:

$\frac{\partial \left( \left( \frac{u}{\theta} \right) ^n \right)} {\partial u} = \frac{n u^{n-1}}{\theta ^ n}$

Thus, for $0 < u < \theta$ the pdf of $\hat{\theta}_{MLE}}{2 \theta}$ is $\frac{n u^{n-1}}{\theta ^ n}$

(B):

To determine the sampling distribution of $\hat{\theta}_{MOM}$, first note the following:

For $i = 1, 2, ...$

$X_i \sim Unif(0, \theta)$

$Y_i \sim Unif(0, 1)$

$U = \sum \limits_{i=1}^{n} {Y_i} \sim IrwinHall(n)$

The follows holds between X and Y:

$\frac{1}{\theta} X \equiv Y \sim Unif(0,1)$

It then follows that:

$\sum \limits_{i=1}^{n} {Y_i} = \frac{1}{\theta} \sum \limits_{i=1}^{n} {X_i} \sim IrwinHall(n)$

Note:

$\hat{\theta}_{MOM} = 2 \bar{X} = 2 \frac{1}{n} \sum \limits_{i=1}^{n} {X_i}$

Thus:

$\frac{\hat{\theta}_{MOM}}{\theta} = \frac{2}{\theta n} \sum \limits_{i=1}^{n} {X_i} = \frac{2}{\theta n}U \sim \frac{2}{n}IrwinHall(n)$

It then follows, removing the denominator from the above relation, that:

$\hat{\theta}_{MOM} \sim \frac{2 \theta}{n} IrwinHall(n)$

Or, put differently:

$\frac{n \hat{\theta}_{MOM}}{2 \theta} \sim IrwinHall(n)$

### Approximation

Via application of the CLT, we may note that the IrwinHall(n) distribution converges to the standard normal as $n \rightarrow \infty$. Thus, bearing in mind the above relations, we may say that as $n \rightarrow \infty$, $\frac{n \hat{\theta}_{MOM}}{2 \theta} \sim N(0, 1)$, or phrase the relation alternatively in terms of $\hat{\theta}_{MOM}$

(C):

Similar to part (A), note the cdf of $\hat{\theta}_{adj}$ is given as follows:

$$\begin{aligned}
f\left( u \right)  = \begin{Bmatrix} 
0 \ for\ u \leq \theta \\
\frac{n+1}{n} \left(\frac{u}{\theta}\right)^n \ for\ 0 < u < \theta \\
1 \ for\ u \geq \theta
\end{Bmatrix}
\end{aligned}$$

Thus, to determine the pdf of $\hat{\theta}_{MLE}}{2 \theta}$, we derive the above equation with respect to u. Thus gives us:

$\frac{\partial \left( \frac{n+1}{n} \left( \frac{u}{\theta} \right) ^n \right)} {\partial u} = \frac{ \left(n +1\right) u^{n-1}}{\theta ^ n}$

Thus, for $0 < u < \theta$ the pdf of $\hat{\theta}_{adj}}{2 \theta}$ is $\frac{ \left(n +1\right) u^{n-1}}{\theta ^ n}$


***6.*** Create a plot of the sampling distribution of each estimator using $n = 10$. Construct the empirical distribution via simulation and overlay the appropriate exact or approximate analytical form (each plot should be a curve overlayed on a histogram. See slides.)

```{r echo=TRUE, fig.heigh=3}

```

***7.*** Form two different 95% confidence intervals for $\theta$ by using pivotal statistics inspired by each estimator.

(1):

Idea of pivotal

Note due to similarity of construction, our pivotal statistic for $\hat{\theta}_{MLE}$ and $\hat{\theta}_{adj}$ are the same. 

$\frac{max(X_i)}{\theta}$

(2):

Idea of pivotal
$\frac{n \hat{\theta}_{MOM}}{2\theta}$

(C):



