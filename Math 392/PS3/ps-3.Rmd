---
title: "MATH 392 Problem Set 3"
author: "Sam D. Olson"
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = FALSE, fig.align = "center", 
                      message = FALSE, warning = FALSE)
library(tidyverse)
library(knitr)
library(here)
```

## Note on PS3

### Time Taken
Total Time on Exercises from the Book: X hours
Solving Exercises from the Book: 4 hours
LaTeX writeup of Exercises from the Book: 2 hours (at 9:11am)

### Additional Note
This Problem Set does not contain the Case Study. It will be turned in on Monday, 2/17/2020.

## Exercises from the book

**7.5**: 5

Let $y = \sum\limits_{i}^{n}{x_i}$

Additionally, denote: $\bar{x} = x_1, ..., x_n$

Then, for $y > 0$ the likelihood function is given by:

$$f_n\left(\bar{x} \mid \theta \right) = \frac{e^{-n \theta} \theta^{y}} { \prod\limits_{i=1}^{n} {x_i !}}$$

Let $L\left( \theta \right) = log \left( f_n\left(\bar{x} \mid \theta \right) \right)$

Note that $\prod\limits_{i=1}^{n} {x_i !}$ does not depend on $\theta$, as such, let $\psi = \prod\limits_{i=1}^{n} {x_i !}$

Then we may write:

$L\left( \theta \right) = -n \theta + y log (\theta) - \psi$

Then we may take the derivative of $L\left( \theta \right)$ with respect to $\theta$, giving us:

$$\frac{\partial {L\left( \theta \right)}}{\partial \theta} = -n + \frac{y}{\theta}$$

To determine the M.L.E. of $\theta$, we maximize this equation and set it equal to zero, such that:

$$0 = -n + \frac{y}{\theta} \rightarrow n = \frac{y}{\theta} \rightarrow \hat{\theta} = \frac{y}{n}$$
(A): Thus, the M.L.E. of $\theta$ is $\hat{\theta} = \frac{y} {n} = \bar{x}_n$.

(B): If $y=0$, and if $\theta \neq 0$ ($\theta>0$), then $\theta=0$ is not in the parameter space, $\Omega$, then the M.L.E. of $\theta$ does not exist.  

For additional commentary, we may note that if $y=0$, then the likelihood function $f_n\left(\bar{x} \mid \theta \right)$ is a decreasing function of $\theta$. As such, the maximum of the derivative of $L \left( \theta \right)$ occurs when $\theta = 0$. However, $\theta > 0$ is a condition of $\theta$, and as such, the M.L.E. of $\theta$ does not exist. 

**7.5**: 8

Let $y = \sum\limits_{i}^{n}{x_i}$

Additionally, denote: $\bar{x} = x_1, ..., x_n$

We note the likelihood function, $f_n\left(\bar{x} \mid \theta \right)$, as:

$$\begin{aligned}
f_n\left(\bar{x} \mid \theta \right) =\begin{Bmatrix} 
e^{\theta - y} \ for\ x > \theta \\
0 \ for\ x \leq \theta
\end{Bmatrix}
\end{aligned}$$

Let $L \left( \theta \right) = log \left( f_n\left(\bar{x} \mid \theta \right) \right)$

Then, for $x > \theta$, we may write:

$L \left( \theta \right) = \theta - y$

Then we may take the derivative of $L\left( \theta \right)$ with respect to $\theta$, setting it equal to $0$ to maximize the likelihood function, giving us: 

$$\frac{\partial {L\left( \theta \right)}}{\partial \theta} = 0 \rightarrow 1 = 0$$

(A): As this is markedly false, we conclude the M.L.E. of $\theta$ does not exist. 

Alternatively, we may not that $\forall x_i \in \bar{x}, f_n\left(\bar{x} \mid \theta \right)$ will be maximized when $\theta$ is maximized (while holding the relation $\theta < x$). As such, $\theta < min(x_1, ..., x_n)$. However, due to the **strict inequality**, the maximum $\theta = min(x_1, ..., x_n)$ cannot be used as an estimate of $\theta$. As such, the M.L.E. of $\theta$ does not exist.  

We may take advantage of the example provided in the book to create another version of the p.d.f. for which the M.L.E. of $\theta$ will exist. To do so, let the **strict inequality** be swapped with the **weak inequality**, such that we may write:

$$\begin{aligned}
f\left({x} \mid \theta \right) =\begin{Bmatrix} 
e^{\theta - y} \ for\ x \geq \theta \\
0 \ for\ x < \theta
\end{Bmatrix}
\end{aligned}$$

We may then note the likelihood function of this p.d.f as:

$$\begin{aligned}
f_n\left(\bar{x} \mid \theta \right) =\begin{Bmatrix} 
e^{\theta - y} \ for\ x \geq \theta \\
0 \ for\ x < \theta
\end{Bmatrix}
\end{aligned}$$

We may then note: The likelihood function will be non-zero for $\theta \leq min(x_1, ..., x_n)$. 

(B): Thus, the M.L.E. of $\theta$, $\hat{\theta} = min(x_1, ..., x_n)$

**7.5**: 11

Note, as we are dealing with the interval $[\theta_1, \theta_2]$, we know $\theta_1 \leq \theta_2$

We may then write the p.d.f. of each x_i as:

$$\begin{aligned}
f\left( x_i \mid \theta_1, \theta_2 \right) =\begin{Bmatrix} 
\frac{1}{\theta_2 - \theta_1} \ for\ \theta_1 \leq x \leq \theta_2\\
0 \ \ otherwise
\end{Bmatrix}
\end{aligned}$$

Letting $\bar{x} = x_1, ..., x_n$, we may then write the likelihood function as: 

$$\begin{aligned}
f_n\left( \bar{x} \mid \theta_1, \theta_2 \right) =\begin{Bmatrix} 
\frac{1}{\left(\theta_2 - \theta_1\right)}^{n} \ for\ \theta_1 \leq min(x_1, ..., x_n) \leq max (x_1, ..., x_n) \leq \theta_2\\
0 \ \ otherwise
\end{Bmatrix}
\end{aligned}$$

We may then note that $f_n\left( \bar{x} \mid \theta_1, \theta_2 \right)$ is maximized when $\theta_2 - \theta_1$ is minimized. 

Thus, we may note:

$min(\theta_2) = max(x_1, ..., x_n)$

And

$max(\theta_1) = min (x_1, ..., x_n)$

Thus, the M.L.E. of $\theta$ may be written: $\theta = (\hat{\theta_1} = min(x_1, ..., x_n), \hat{\theta_2} = max(x_1, ..., x_n))$.

**7.5**: 12

Let $\theta = \theta_1, ..., \theta_n$

Note the following relations for use later:
$k \geq 2$, $0 \leq \theta_i \leq 1$, $\theta_1+...+\theta_k=1$, and $n_1+...+n_k=n$

Then, the likelihood function is given by:

$$f_n\left(\bar{x} \mid \theta_1, ..., \theta_k \right) = \prod\limits_{i=1}^{k}{\theta_{i}}^{n_i} = {\theta_1}^{n_1} ... {\theta_k}^{n_k}$$

For ease of computation, let $L\left( \theta \right) = log \left( f_n\left(\bar{x} \mid \theta_1, ..., \theta_k \right) \right)$

Note that: $\theta_k = 1- \sum\limits_{i=1}^{k-1}{\theta_i}$

Then we may write:

$L\left( \theta \right) = \sum\limits_{i=1}^{k} n_i log(\theta_i)$

Then we may take the derivative with respect to $\theta_i$, giving us:

$$\frac{\partial {L\left( \theta \right)}}{\partial \theta_i} = \frac{\partial \left( n_k\left(1-\sum\limits_{i=1}^{k-1}{\theta_i}\right)+{\sum\limits_{i=1}^{k-1} n_i log(\theta_i)}\right)}{\partial \theta_i} $$

This evaluates to:

$$\frac{\partial {L\left( \theta \right)}}{\partial \theta_i} = -\frac{n_k}{\left(1-\sum\limits_{i=1}^{k-1}{\theta_i}\right)} + \frac{n_i}{\theta_i} = -\frac{n_k}{\theta_k}+\frac{n_i}{\theta_i}$$

To determine the M.L.E. of $\theta_i$, we maximize this equation and set it equal to zero, such that:

$0 = -\frac{n_k}{\theta_k}+\frac{n_i}{\theta_i} \rightarrow \frac{n_k}{\theta_k} = \frac{n_i}{\theta_i}$ for $i=1, ..., k-1$

For $i=1, ..., k$, if we set $\theta_i = \alpha n_i$, then it follows: 

$1 = \sum\limits_{i=1}^{k}{\theta_i} = \sum\limits_{i=1}^{k}{\alpha n_i} = \alpha \sum\limits_{i=1}^{k}{n_i}$

Noting that $\sum\limits_{i=1}^{n}{n_i}= n$, we may then say:

$1 = \alpha n \rightarrow \alpha = \frac{1}{n}$

We may then write, for $i=1, ..., k$, the M.L.E. of $\theta_i$ is:

$\hat{\theta_i} = \alpha n_i = n_i /n$

Intuitively, and similar to other applications of the M.L.E., this results says that the M.L.E. of the individuals of the type i is the proportion of individuals of type i. 

**7.6**: 3



**7.6**: 12



**7.6**: 14



**7.6**: 23


