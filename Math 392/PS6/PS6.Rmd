---
title: "MATH 392 Problem Set 6"
author: "Sam D. Olson"
output: 
  pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = FALSE, fig.align = "center", 
                      message = FALSE, warning = FALSE)
library(tidyverse)
library(knitr)
library(here)
```


## Exercise from the book

**9.1 #1**

***Q:*** Let X have the exponential distribution with parameter $\beta$. Suppose that we wish to test the hypotheses $H_0 : \beta \geq 1$ versus $H_1 : \beta < 1$. Consider the test procedure $\delta$ that rejects $H_0$ if $X \geq 1$. 

**(A)**

Determine the power function of the test.  

Let $\delta$ demote a test, specifically: 

$\delta$ :{Reject $H_0$ when $X \geq 1$}

Then for $\beta >0$ we note the power function as follows, $\pi \left( \beta \mid \delta \right) = Pr \left(X \geq 1 \mid \beta \right) = e^{-\beta}$

**(B)**

Compute the size of the test. 

By the definition of size, we have: 

$size(\delta) = sup_{\beta \geq 1} \pi \left( \beta \mid \delta \right)$

Note the power function from **(A)**, $\pi \left( \beta \mid \delta \right)  = e^{-\beta}$, such that $\pi \left( \beta \mid \delta \right)$ is a decreasing function of $\beta$. Thus, the power function takes a maximum at $\beta = 1$, and we may say: 

The size of the test is $\pi \left( \beta = 1 \mid \delta \right) = e^{-1}$.


**9.1 #2**

***Q:*** Suppose that $X_1, ..., X_n$ form a random sample from the uniform distribution on the interval $[0, \theta]$, and that the following hypotheses are to be tested: 

$H_{0} : \theta \geq 2$

$H_{1} : \theta < 2$

Let $Y_n = max \left(X_1, ..., X_n \right)$ and consider a test procedure such that the critical region contains all the outcomes for which $Y_n \leq 1.5$. 

**(A)**

Determine the power function of the test.  

Note, for $0 < y < \theta$, then $Pr \left(Y_n \leq y\right) = \left( \frac{y}{\theta}\right) ^n$.

Additionally note, if $y \geq \theta$, then $Pr \left(Y_n \leq y\right) = 1$.

Thus, for the stated condition, if $\theta \leq 1.5$, then $\pi(\theta) = Pr(Y_n \leq 1.5) = 1$. 

And if $\theta > 1.5$, then $\pi(\theta) = Pr(Y_n \leq 1.5) = \left( \frac{1.5}{\theta} \right) ^n$

**(B)**

Determine the size of the test. 

The size of the test, $size(\delta) = \alpha = sup_{\theta \geq 2} \pi \left(\theta \right) = sup_{\theta \geq 2} \left(\frac{1.5}{\theta} \right)^n = \left(\frac{1.5}{2} \right)^n = \left(\frac{3}{4} \right)^n$.

**9.1 #14**

Plus: plot power functions in R

***Q:*** Let $X_1, ..., X_n$ be i.i.d. with exponential distribution with parameter $\theta$. Suppose that we wish to test the hypotheses: 

$H_{0} : \theta \geq \theta_0$

$H_{1} : \theta < \theta_0$

Let $X = \sum \limits_{i=1}^{n} {X_i}$. Let $\delta_c$ be the test that rejects $H_0$ if $X \geq c$.

**(A)**

Show that $\pi \left( \theta \mid \delta_c \right)$ is a decreasing function of $\theta$ 

We take advantage of the second parameter of the Gamma distribution being being a location parameter. Thus we may note: 

The distribution of $X \sim Gamma\left( n, \theta\right)$ 

The distribution of $Y = \theta X \sim Gamma\left( n , 1\right)$ 

Let us then note the c.d.f. of Y, $G_{n}$. 

We then note the power function of $\delta_c$ as: 

$\pi\left( \theta \mid \delta_c\right) = Pr(X \geq c\mid \theta) = Pr(Y \geq c\theta \mid \theta) = 1- G_{n} \left( c\theta \right)$

Note that $G_{n}$ is an increasing function of $\theta$ and $c \theta$ is an increasing function of $\theta$, making $\pi\left( \theta \mid \delta_c\right) = 1- G_{n} \left( c\theta \right)$ a decreasing function of $\theta$. 

**(B)**

Find c in order to make $\delta_c$ have size $\alpha_0$. 

Note, to find c we set the above power function equal to the size, giving us: 

$\pi\left( \theta_0 \mid \delta_c\right) = 1- G_{n} \left( c\theta_0 \right) = \alpha_0$

Taking the inverse c.d.f. allows us to solve for c, giving us: 

$1 - \alpha_0 = G_{n} \left( c\theta_0 \right)$

$G_{n}^{-1} \left(1 - \alpha_0 \right) = c \theta_0$

$c = \frac{G_{n}^{-1} \left(1 - \alpha_0 \right)}{\theta_0}$

**(C)**

Let $\theta_0 = 2$, $n=1$, and $\alpha_0 = 0.1$. Find the precise form of the test $\delta_c$ and sketch its power function. 

We have: 

$G_n (y) = 1 - e^{-y}$, 

$G_n ^{-1} = -log(1-p)$ 

Using our formulation of c from **(B)** gives us: 

$c = \frac{- log(0.1)}{2} \approx 1.15$

We may then plot the relationship between $\theta$ and the power function, given below.

```{r echo=TRUE, eval=TRUE, fig.heigh=3}
theta <- (0:6)

power <- rep(NA, 7) 

for(i in 0:6) {
  power[i] <- exp((-1)* 1.15 * theta[i])
}

df <- data.frame(theta,power)

ggplot(df, aes(theta)) +                  
  geom_line(aes(y=power), colour="black") + 
  geom_text(x=1.5, y=0.4, label="Power") +
  ggtitle("Power as a Function of Theta") +
  geom_text(x=2.5, y=0.145, label="Size") +
  geom_hline(yintercept=0.1, linetype="dashed", color = "red") +
  labs(y="Power", x = "Theta")
```

**9.2 #2**

***Q:*** Consider two p.d.f.'s $f_{0} \left(x\right)$ and $f_{1} \left(x\right)$ that are defined as follows: 

$$\begin{aligned}
f_{0} \left(x\right) = \begin{Bmatrix} 
{1} \ for\ 0 \leq x \leq 1 \\
0 \ \ otherwise
\end{Bmatrix}
\end{aligned}$$

and 

$$\begin{aligned}
f_{1} \left(x\right) = \begin{Bmatrix} 
{2x} \ for\ 0 \leq x \leq 1 \\
0 \ \ otherwise
\end{Bmatrix}
\end{aligned}$$

Suppose that a single observation X is taken from a distribution for which the p.d.f. $f(x)$ is either $f_{0} \left(x\right)$ or $f_{1} \left(x\right)$, and the following simple hypotheses are to be tested: 

$H_{0} : f(x) =  f_{0} \left(x\right)$

$H_{1} : f(x) =  f_{0} \left(x\right)$

**(A)**

Describe a test procedure $\delta$ for which the value of $\alpha \left( \delta \right) + 2\beta \left(\delta \right)$ is a minimum. 

Using Thm. 9.2.1 with $a-1, b=2$, we have: 

$\delta$ :{Do not reject $H_0$ if $\frac{f_1(x)}{f_0(x)} < \frac{1}{2}$}

Note, as  $\frac{f_1(x)}{f_0(x)} = 2x \rightarrow 2x < \frac{1}{2}$

Thus we may say the procedure is: Do not reject $H_0$ if $x < \frac{1}{4}$, and likewise to reject $H_0$ if $x > \frac{1}{4}$. 

**(B)**

Determine the minimum value of $\alpha \left( \delta \right) + 2\beta \left(\delta \right)$ attained by that procedure.  

Note the following formulations: 

$\alpha (\delta) = Pr( Reject \space H_0 \mid f_0) = \int \limits_{\frac{1}{4}}^{1} {f_0(x) dx } = \frac{3}{4}$

$\beta (\delta) = Pr( Fail \space to \space  Reject \space  H_0 \mid f_1) = \int \limits_{0}^{\frac{1}{4}} {2x dx } = \frac{1}{16}$

Therefore, we have: 

$\alpha \left( \delta \right) + 2\beta \left(\delta \right) = \frac{3}{4} + 2\frac{1}{16} = \frac{7}{8}$

**9.2 #3**

***Q:*** Consider again the conditions of Exercise 2 (9.2.2), but suppose now that it is desired to find a test procedure for which the value of $3\alpha \left( \delta \right) + \beta \left(\delta \right)$ is a minimum. 

**(A)**

Determine the procedure.  

Using Thm. 9.2.1. with $a=3, b=1$, we have:

$\delta$ :{Do not reject $H_0$ if $\frac{f_1(x)}{f_0(x)} < 3 \rightarrow 2x < 3$}

Since all values of X lie in the interval (0, 1), the optimal procedure is to not reject $H_0$ for every possible observed value (as $max(X)=1 \rightarrow 2 \cdot max(X)=2 < 3$).

**(B)**

Determine the minimum value of $3\alpha \left( \delta \right) + \beta \left(\delta \right)$ attained by the procedure. 

Since we never reject $H_0$ under the stated conditions, we have:

$\alpha (\delta) = 0, \beta(\delta)=1$ and we thus have:

$3\alpha (\delta) + \beta(\delta) = 1$

**9.2 #10**

***Q:*** Suppose that $X_1, ..., X_n$ form a random sample from the Poisson distribution with unknown mean $\lambda$. Let $\lambda_0$ and $\lambda_1$ be specified values such that $\lambda_1 > \lambda_0 > 0$, and suppose that it is desired to test the following simple hypotheses:  

$H_{0} : \lambda = \lambda_0$

$H_{1} : \lambda = \lambda_1$

**(A)**

Show that the value of $\alpha \left( \delta \right) + \beta \left(\delta \right)$ is minimized by a test procedure which rejects $H_0$ when $\bar{X}_n > c$.

Applying Thm. 9.2.1 with $a = b = 1$, we know the optimal test procedure is to reject $H_0$ if $\frac{f_1(\bar{X})}{f_0(\bar{X})} > 1$. 

Let $y = \sum \limits_{i=1}^{n} {x_i}$

We may note the joint p.f. of the data as:

$f_n(\bar{X} \mid \lambda) = f_n(X_1 \mid \lambda) ...f_n(X_n \mid \lambda) = \frac{e^{-\lambda} \lambda^{x_{1}}}{x_1!} ... \frac{e^{-\lambda} \lambda^{x_{n}}}{x_n!} = \frac{e^{-n\lambda} \lambda^{y}}{\prod \limits_{i=1}^{n} x_i!}$

Thus we have: 

$f_0(\bar{X}) = \frac{e^{-n\lambda_0} \lambda_0^y}{\prod \limits_{i=1}^{n} {x_i!}}$

$f_1(\bar{X}) = \frac{e^{-n\lambda_1} \lambda_1^y}{\prod \limits_{i=1}^{n} {x_i!}}$

It then follows that: 

$\frac{f_1(\bar{X})} {f_0(\bar{X})} = e^{-n \left( \lambda_2 - \lambda_1 \right)} \cdot \left(\frac{\lambda_2}{\lambda_1}\right)^{y}$

Taking the log of this gives us: 

$log \left( \frac{f_1(\bar{X})} {f_0(\bar{X})} \right) = y  \space log\left(\frac{\lambda_2}{\lambda_1}\right) -n \left( \lambda_1 - \lambda_0 \right)$

Note by construction $\lambda_1 > \lambda_0$. Therefore, we know: 

$\frac{f_1(\bar{X})} {f_0(\bar{X})} > 1 \space \space iff \space \space \frac{y}{n} > \frac{(\lambda_1-\lambda_0)}{(log \space \lambda_1 - log \space \lambda_0)}$

**(B)**

Find the value of c. 

Note that $\bar{x}_n = \frac{y}{n}$. 

Utilizing the above and the formulations from **(A)**, we want a test procedure which rejects $H_0$ when $\bar{X}_n > c$, giving us: 
$c = \frac{(\lambda_1-\lambda_0)}{(log \space \lambda_1 - log \space \lambda_0)}$

**(C)**

For $\lambda_0 = \frac{1}{4}$, $\lambda_1 = \frac{1}{2}$, and $n=20$, determine the minimum value of $\alpha \left( \delta \right) + \beta \left(\delta \right)$ that can be attained. 

If $H_0$ is true, then $Y = \sum \limits_{i=1}^{n} {X_i} \sim Poisson \left( \lambda_0\right)$ with mean $n\lambda_0$

And if $H_1$ is true, then $Y = \sum \limits_{i=1}^{n} {X_i} \sim Poisson \left( \lambda_1\right)$ with mean $n\lambda_1$

Noting the specified conditions, $\lambda_0 = \frac{1}{4}$, $\lambda_1 = \frac{1}{2}$, and $n=20$, we have: 

$\frac{n (\lambda_1-\lambda_0)}{(log \space \lambda_1 - log \space \lambda_0)} = \frac{20 \cdot 0.25}{log(0.50) - log(0.25)} \approx 7.21$

We use this value in the following computations, which we are able to evaluate using Poisson distribution tables, but first we must path our tithe and say:

***All Praise the Glorious Tables in the Back of the Book***

With tithes out of the way, note to evaluate $\alpha (delta)$ we use a Poisson distribution with mean $5 = 20 \cdot 0.25$. 

$\alpha (\delta) = Pr(Y > 7.21 \mid H_0) = Pr(Y \geq 8 \mid H_0) \approx  0.13$

Similarly, to evaluate $\beta (delta)$ we use a Poisson distribution with mean $10 = 20 \cdot 0.50$.

$\beta (\delta) = Pr(Y \leq 7.21 \mid H_1) = Pr(Y \leq 7 \mid H_1) \approx 0.22$

Combining these together, we have: 

$\alpha \left( \delta \right) + \beta \left(\delta \right) = 0.13 + 0.22 = 0.35$ as the minimum value that can be attained. 

**9.3 #1**

***Q:*** Suppose that $X_1, ..., X_n$ form a random sample from the Poisson distribution with unknown mean $\lambda$ $\left( \lambda > 0 \right)$. Show that the joint p.f. of $X_1, ..., X_n$ has a monotone likelihood ratio in the statistic $\sum \limits_{i=1}^{n} {X_i}$.

Let $y = \sum \limits_{i=1}^{n} {x_i}$

We may note the joint p.f. of the data as: 

$f_n(\bar{X} \mid \lambda) = f_n(X_1 \mid \lambda) ...f_n(X_n \mid \lambda) = \frac{e^{-\lambda} \lambda^{x_{1}}}{x_1!} ... \frac{e^{-\lambda} \lambda^{x_{n}}}{x_n!} = \frac{e^{-n\lambda} \lambda^{y}}{\prod \limits_{i=1}^{n} x_i!}$

Thus, for $0 < \lambda_1 < \lambda_2$, we have: 

$\frac{f_n(\bar{X} \mid \lambda_2)}{f_n(\bar{X} \mid \lambda_1)} = \frac{\frac{e^{-n\lambda_2} \lambda_2^{y}}{\prod \limits_{i=1}^{n} x_i!}}{\frac{e^{-n\lambda_1} \lambda_1^{y}}{\prod \limits_{i=1}^{n} x_i!}}$

Noting the factorials of $x_i$ cancel out, we then have: 

$\frac{f_n(\bar{X} \mid \lambda_2)}{f_n(\bar{X} \mid \lambda_1)} = e^{-n \left( \lambda_2 - \lambda_1 \right)} \cdot \left(\frac{\lambda_2}{\lambda_1}\right)^{y}$

Note, as $0 < \lambda_1 < \lambda_2$, this is an increasing function of y and we conclude the joint p.f. of $X_1, ..., X_n$ has a monotone likelihood ratio in the statistic $\sum \limits_{i=1}^{n} {X_i}$. 

**9.3 #2**

***Q:*** Suppose that $X_1, ..., X_n$ form a random sample from the normal distribution with known mean $\mu$ and unknown variance $\sigma^2$ $\left( \sigma^2 > 0 \right)$. Show that the joint p.d.f. of $X_1, ..., X_n$ has a monotone likelihood ratio in the statistic $\sum \limits_{i=1}^{n} {\left(X_i - \mu\right)^2}$. 

Let $y = \sum \limits_{i=1}^{n} {\left(x_i - \mu\right)^2}$

We may then note the joint p.d.f. of the data as: 

$f_n(\bar{X} \mid \sigma^2) = f_n(X_1 \mid \sigma^2) ... f_n(X_n \mid \sigma^2) = \frac{1}{\sqrt{\left( 2 \pi \sigma^2 \right)}} \cdot e^{\frac{-x_1}{2 \sigma^2}} ... \frac{1}{\sqrt{\left( 2 \pi \sigma^2 \right)}} \cdot e^{\frac{-x_n}{2 \sigma^2}}$

Simplifying this expression gives us: 

$f_n(\bar{X} \mid \sigma^2) = \frac{1}{\sqrt{\left( 2 \pi \sigma^2 \right)}^n} \cdot e^{\frac{-y}{2 \sigma^2}}$

Thus, noting the condition $0 < \sigma_1^2 < \sigma_2^2$, we have: 

$\frac{f_n(\bar{X} \mid \sigma_2^2)}{f_n(\bar{X} \mid \sigma_1^2)} = \frac{\frac{1}{\sqrt{\left( 2 \pi \sigma_2^2 \right)}^n} \cdot e^{\frac{-y}{2 \sigma_2^2}}} {\frac{1}{\sqrt{\left( 2 \pi \sigma_1^2 \right)}^n} \cdot e^{\frac{-y}{2 \sigma_1^2}}}$

Simplifying this expression then gives us:m 

$\frac{f_n(\bar{X} \mid \sigma_2^2)}{f_n(\bar{X} \mid \sigma_1^2)} = \left(\frac{\sigma_1}{\sigma_2}\right)^n \cdot e^{\frac{y}{2} \cdot \left(\frac{1}{\sigma_1^2} - \frac{1}{\sigma_2^2} \right)}$

Noting the prior condition that $0 < \sigma_1^2 < \sigma_2^2 \rightarrow \frac{1}{\sigma_1^2} > \frac{1}{\sigma_2^2}$, and we may say the above ($\frac{f_n(\bar{X} \mid \sigma_2^2)}{f_n(\bar{X} \mid \sigma_1^2)}$) is an increasing function of y. 

We may then conclude that the joint p.d.f. of $X_1, ..., X_n$ has a monotone likelihood ratio in the statistic $\sum \limits_{i=1}^{n} {\left(X_i - \mu\right)^2}$.

**9.3 #13**

***Q:*** Suppose that four observations are taken at random from the normal distribution with unknown mean $\mu$ and known variance 1. Suppose also that the following hypotheses are to be tested: 

$H_{0} : \mu \geq 10$

$H_{1} : \mu < 10$

**(A)**

Determine a UMP test at the level of significance $\alpha_{0} = 0.1$

Note: via Exercise 9.3.12, a test which rejects $H_0$ when $\bar{X}_n \leq c$ will be a UMP test. 

Thus, for a significance level $\alpha_{0} = 0.1$, we want to choose a value of c that satisfies: 

$Pr(\bar{X}_n \leq c \mid \mu =10) = 0.1$

As we have four observations, we have $n=4$. For $\mu = 10$, let us note the variable z with the standard normal distribution, specifically: 

$z = 2(\bar{X}_n - 10)$

Referencing the above probability formula, we thus have: 

$Pr(\bar{X}_n \leq c \mid \mu =10) = Pr(Z \leq 2(c - 10)) = 0.1$

As the standard normal distribution is a known distribution, we may take advantage of the known formulation, namely: 

$Pr(Z \leq 2(c - 10)) = 0.1 = Pr(Z \leq -1.28)$

Solving for c gives us: 

$2(c-10) = -1.28 \rightarrow c = \frac{18.72}{2} = 9.36$

**(B)**

Determine the power of this test when $\mu = 9$. 

Using a similar formulation to **(A)**, we note: 

$z = 2(\bar{X}_n - 9) ~ N(0,1)$ (the standard normal distribution). 
We then formulate the power of the test as: 

$Pr(\bar{X}_n \leq 9.36 \mid \mu = 9) = Pr(Z \leq 0.718) = \Phi (0.718) \approx 0.76$

**(C)**

Determine the probability of not rejecting $H_0$ if $\mu = 11$.

Using a similar formulation to **(A)** and **(A)**, we note: 

$z = 2(\bar{X}_n - 11) ~ N(0,1)$ (the standard normal distribution). 

We then formulate the probability of rejecting $H_0$ as:  

$Pr(\bar{X}_n \leq 9.36 \mid \mu = 11) = Pr(Z \geq -3.282) = Pr(Z \leq 3.282) = \Phi (3.282) \approx 0.9995$