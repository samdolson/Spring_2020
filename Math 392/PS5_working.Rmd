---
title: "MATH 392 Problem Set 5"
author: "Sam D. Olson"
output: 
  pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = FALSE, fig.align = "center", 
                      message = FALSE, warning = FALSE)
library(tidyverse)
library(knitr)
library(here)
```

### Time Spent on PS4
Total: $7$ hours
Exercises: $4 \frac{1}{2}$ hours
Case Study: $2 \frac{1}{2}$ hours

## Exercises from the book

**8.1 #1**

Random Sample $X_1, ..., X_n$ taken from the Uniform distribution on the interval $[0, \theta]$

$\forall {\theta}$, how large must a random sample be taken in order that:

$PR\left(\mid max\left(X_1, ..., X_n\right) - \theta \mid \leq 0.1 \theta \right) \geq 0.95$

Let $\hat{X} = max(X_1, ..., X_n)$

Note the c.d.f. of $\hat{X}=max(X_1, ..., X_n)$ is

$$\begin{aligned}
f\left( u \right)  = \begin{Bmatrix} 
0 \ for\ u \leq \theta \\
\left(\frac{u}{\theta}\right)^n \ for\ 0 < u < \theta \\
1 \ for\ u \geq \theta
\end{Bmatrix}
\end{aligned}$$

Note since $U \leq \theta$ with probability 1, then $\mid U - \theta \mid \leq 0.1 \theta$ is the same as the event that $U \geq 0.9\theta$.

The probability of this event is $1 - F\left( 0.9 \theta \right) = 1 - 0.9^n$ To satisfy the condition, we have: 

$\rightarrow 0.95 = 1 - 0.9^n \rightarrow 0.95^n = 0.05$

We take the log of each side to solve for n, giving us:

$log (0.95) n = log (0.05) \rightarrow n = \frac{log(0.05)}{log(0.95)}$

Evaluating this gives us $n=28.43$. Thus we note that as n increases, $1 - 0.9^n$ decreases, such that we need a sample of at least 29 observations to satisfy the condition. 

**8.1 #3**

For the conditions of Exercise 2, how large a random sample must be taken in order that $E_\theta \left( \mid \bar{X}_n - \theta \mid \right) \leq 0.1$ $\forall {\theta}$

First, let us note the necessary conditions from Exercise 2 for our purposes. 

We know a random sample is to be taken from the normal distribution with unknown mean $\theta$ and standard deviation 2. 

It then follows that $E_\theta \left( \mid \bar{X}_n - \theta \mid ^2 \right) = Var_\theta \left( \bar{X}_n \right) = \frac{4}{n}$

We then know $\bar{X}_n$ has the normal distribution with mean $\theta$ and variance $\frac{4}{n}$

Thus, we may construct the random variable $Z= \frac{ \left(\bar{X}_n - \theta \right)} {\frac{2}{\sqrt{n}}}$ that fits the standard normal distribution.

Thus we have:

$$\begin {aligned} 
  E_\theta \left( \mid \bar{X}_n - \theta \mid\right) = 
  \frac {2}{\sqrt{n}} E_\theta \left( \mid Z \mid \right) \\ 
  & = \frac {2}{\sqrt{n}} \int\limits_{- \infty}^{\infty} {\mid z \mid \frac{1}{\sqrt{2 \pi}}} e^ {\frac{-z^2}{2}} dz
  \end {aligned}$$

Taking advantage of symmetry, we can simplify this to:

$$\begin {aligned} 
  E_\theta \left( \mid \bar{X}_n - \theta \mid\right) = 
  2 \sqrt \frac {2}{n \pi} \int\limits_{0}^{\infty} {z e^ {\frac{-z^2}{2}} dz} \\ 
  & = 2 \sqrt \frac {2}{n \pi}
  \end {aligned}$$

However, $2 \sqrt \frac {2}{n \pi} \leq 0.1$ if $n \geq \frac{800}{\pi}$

Evaluating this fraction, we have $n \geq 254.6$, thus we note as n increases the values of $2 \sqrt \frac {2}{n \pi}$ decreases. 

As such, we need a random sample of at least size 255 to satisfy the stated condition. 

**8.2 #2**
Find the mode of the $\chi^{2}$ distribution with m degrees of freedom $\left( m = 1, 2,... \right)$

The mode will be the value of x at which the p.d.f. $f(x)$ is at a maximum. This applies similarly to the log of $f(x)$. 

The p.d.f. of $f(x)$ is given as:

$f(x)= \frac{1}{2^\frac{m}{2} \Gamma\left(\frac{m}{2}\right)}  x^{\frac{m}{2} - 1} e^{-\frac{x}{2}}$

Note, the first part of this equation does not include x, thus we may note $\phi$ as some constant in taking the log, giving us:

Thus $log \left(f(x) \right)= \phi + \left( \frac{m}{2} - 1 \right) log (x) - \frac{x}{2}$

To solve for the mode, we then set $m=1,2,3,4$, and attempt to maximize (when possible). 

Generally, we may maximize by deriving with respect to x and setting the equation equal to 0. This only allows us to solve for $m \geq 2$, as is shown below. 

### General Form

$0 = \frac{\partial log \left( f(x) \right)}{\partial x} = \frac{1}{x} \left(\frac{m}{2} - 1 \right) - \frac{1}{2}$

$\rightarrow \frac{1}{2} x = \frac{m}{2} - 1 \rightarrow x = m - 2$

However, the p.d.f. is not defined on negative numbers, such that this formula does not yield answers within the parameter space for $m=1$, -1. 

### Specific Commentary on m=1, 2, 3, 4

For $m=1$, this function is strictly decreasing and increases without bound as x approaches 0. Hence the greatest density will be had at $x=0$ for $m=1$.  

For $m=2$, this function is strictly decreases and attains its maximum value when $x=0$

If $m=3$, the value of x at which the maximum is attained can be found by setting the derivative with respect to x equal to 0. In this instance, $x=m-2$. This holds for $m \geq 3$

Thus, the mode of the $\chi^{2}$ distribution, x, with m degrees of freedom, where $m=1, 2, ...$ is given by the relation:

$$\begin{aligned}
x  = \begin{Bmatrix} 
0 \ for\ m < 3 \ \\
m - 2 \ for\ m \geq 3
\end{Bmatrix}
\end{aligned}$$

**8.2 #3**
Sketch the p.d.f. of the $\chi^2$ distribution with m degrees of freedom for each of the following values of m. Location the mean, the median, and the mode on each sketch.

Note, from the prior Exercise, we are able to calculate the modes of the $\chi^2$ distribution with 1, 2, 3, and 4 degrees of freedom. Namely:

(1): $x_{m=1} = 0$

(2): $x_{m=2} = 0$

(3): $x_{m=3} = 3 - 2 = 1$

(4): $x_{m=4} = 4 - 2 = 2$

Note, the mean of the $\chi^2$ distribution is the degrees of freedom. Hence, we have:

(1): $\bar{x}_{m=1} = 1$

(2): $\bar{x}_{m=2} = 2$

(3): $\bar{x}_{m=3} = 3$

(4): $\bar{x}_{m=4} = 4$

Similarly, to evaluate the medians of each $\chi^2$ distribution, we use the function $qchisq(.5, df=m)$, noting the median is the 50th percentile of the distribution. Thus we have:

```{r, eval=TRUE, echo=TRUE}
qchisq(.5, df=1)
qchisq(.5, df=2)
qchisq(.5, df=3)
qchisq(.5, df=4)
```

(1): $\tilde{x}_{m=1} = qchisq(.5, df=1) = 0.45$

(2): $\tilde{x}_{m=2} = qchisq(.5, df=2) = 1.39$

(3): $\tilde{x}_{m=3} = qchisq(.5, df=3) = 2.37$

(4): $\tilde{x}_{m=4} = qchisq(.5, df=4) = 3.36$

Using the above information, we then make the following plots. Note, red denotes the mode, green denotes the median, and blue denotes the mean. Full disclosure: I couldn't find a consistent working (and aesthetically pleasing) way to display labels on the below vertical lines. Very open to your thoughts. 

``` {r, eval=TRUE, echo=TRUE}
eq = function(x){dchisq(x, df = 1)}
ggplot(data.frame(x=c(-1, 10)), aes(x=x)) + 
  stat_function(fun=eq) + 
  geom_vline(colour="red", xintercept=0) + 
  geom_vline(colour="blue",xintercept=1) + 
  geom_vline(colour="green", xintercept=0.45)+
  labs(title="Chi-Squared with 1 df.") +
  theme_minimal()
```

``` {r, eval=TRUE}
eq = function(x){dchisq(x, df = 2)}
ggplot(data.frame(x=c(-1, 10)), aes(x=x)) + 
  stat_function(fun=eq) + 
  geom_vline(colour="red",xintercept=0) + 
  geom_vline(colour="blue", xintercept=2) + 
  geom_vline(colour="green", xintercept=1.39) +
  geom_text(x=0, y=0, label="test") +
  labs(title="Chi-Squared with 2 df.") +
  theme_minimal()
```

``` {r, eval=TRUE}
eq = function(x){dchisq(x, df = 3)}
ggplot(data.frame(x=c(-1, 10)), aes(x=x)) + 
  stat_function(fun=eq) + 
  geom_vline(colour="red",xintercept=1) + 
  geom_vline(colour="blue", xintercept=3) + 
  geom_vline(colour="green",xintercept=2.37) +
  labs(title="Chi-Squared with 3 df.") +
  theme_minimal()
```

``` {r, eval=TRUE}
eq = function(x){dchisq(x, df = 4)}
ggplot(data.frame(x=c(-1, 10)), aes(x=x)) + 
  stat_function(fun=eq) + 
  geom_vline(color="red",xintercept=2) +
  geom_vline(color="blue",xintercept=4) +
  geom_vline(color="green",xintercept=3.36)+
  labs(title="Chi-Squared with 4 df.") +
  theme_minimal()
```

**8.2 #4** 
Suppose that a point $(X,Y)$ is to be chosen at random in the xy-plane, where X and Y are independent random variables and each has the standard normal distribution. 

If a circle is drawn in the xy-plane with its center at the origin, what is the radius of the smallest circle that can be chosen in order for there to be probability 0.99 that the point (X,Y) will lie inside the circle?

Let r denote the radius of the circle. Thus, the point (X,Y) will lie inside the circle if $X^2 + Y^2 < r^2$

Also, $X^2 + Y^2$ has a $\chi^{2}$ distribution with 2 degrees of freedom. 

We may then note the c.d.f. of the $\chi^{2}$ distribution with 2 degrees of freedom as follows:

$F\left(x, m \right) = \frac{ \gamma \left( \frac{m}{2}, \frac{x}{2} \right)}{\Gamma \left( \frac{m}{2} \right)}$

Where $\gamma \left( s , x \right)$ is the lower incomplete gamma function given as:

$\gamma \left( s,x \right) = \int \limits_{0}^{x} t^{s-1} e^{-t} dt$

We may set the c.d.f. equal to 0.99 and solve for the bound. This is, frankly, a bit hectic (though I would be happy to screencap Wolfram Alpha if need be).

Instead, we may use the above formula to check our work, but in the meantime I highly recommend turning to page 859 and looking at $p=0.99$ for the $\chi^{2}$ distribution with 2 degrees of freedom, which gives us a value of 9.210. 

Thus, we have:

$Pr \left( X^2 + Y^2 \leq 9.210 \right) = 0.99$

Therefore, we substitute $r^2$ into the above equation to give us $r^2 \geq 9.210$, noting that r can be as small as $r = \sqrt{9.210} = 3.03$ units.

**8.2 #13**

Prove that the distribution of $\hat{\sigma_0^2}$ in Examples 8.2.1 and 8.2.2 is the gamma distribution with parameters $\frac{n}{2}$ and $\frac {n} {2\sigma^2}$

From the prior examples noted, let $W = \frac{n \hat{\sigma^2}}{\sigma^2}$. Then it follows that W has the $\chi^{2}$ distribution with n degrees of freedom. 

Note, the $\chi^{2}$ distribution with n degrees of freedom is the same as the gamma distribution with parameters $\frac{n}{2}$ and $\frac{1}{2}$. 

Thus, if we multiply a gamma random variable by a constant, we change its distribution to another gamma distribution. Of note, this distribution will have the same first parameter as the initial gamma distribution (in our case $\frac{n}{2}$) and a modified second parameter, $\beta_{new}= \frac{\beta_{old}}{c}$, with some constant c. 

Since $\hat{\sigma^2} = \frac{\sigma^2}{n} W$, we see that the constant c is given by:

$c=\frac{\sigma^2}{n}$

Thus, we may say the distribution of $\hat{\sigma^2}$ is the gamma distribution with parameters $\frac{n}{2}$ and $\frac{n}{2\sigma^2}$

**8.4 #3**
Suppose that five random variables, $X_1, ..., X_n$ are i.i.d. and that each has the standard normal distribution. Determine a constant c such that the random variable

$\frac{c\left(X_1+X_2\right)}{\left(X_3^2 + X_4^2 + X_5^2\right)^\frac{1}{2}}$ 

will have a t distribution.

As each $X_1, ..., X_n$ has the standard normal, $X_1 + X_2$ has the normal distribution with mean 0 and variance 2. 

Therefore, $Y = \frac{\left(X_1 + X_2 \right)}{\sqrt{2}}$ also has a standard normal distribution. 

Additionally, $Z = X_3^2 + X_4^2 + X_5^2$ has the $\chi^{2}$ distribution with 3 degrees of freedom. 

By their construction, Y and Z are independent as they do not share $X_i$ terms.

Therefore, we create a variable $U$ using a product (a division in this case) of Y and Z, noted:

$U= \frac{Y}{\left(\frac{Z}{3}\right)^\frac{1}{2}} = \frac{\left(X_1+X_2\right)}{\sqrt 2 \left(\frac{\left(X_3^2 + X_4^2 + X_5^2\right)}{3}\right)^\frac{1}{2}}$ has the t distribution with 3 degrees of freedom. 

Thus, we may choose a constant $c=\sqrt\frac{3}{2}$, noting $\frac{1}{\frac { \sqrt 2} {\sqrt 3}}= \frac{\sqrt 3 }{\sqrt 2}$, then the given condition will be satisfied. 

\newpage

## Case Study: German Tank Problem

Let's pick up the example that we began in class but make the simplifying assumption that we're studying a process where our sample is drawn from the continuous distribution, $X_1, X_2, \ldots, X_n \sim \textrm{Unif}(0, \theta)$, but we're still interested in estimating $\theta$. The MLE and Method of Moments estimators are the same:

$$\begin{aligned}
\hat{\theta}_{MLE} &= \textrm{max}(X_1, X_2, \ldots, X_n) = X_{max} \\
\hat{\theta}_{MOM} &= 2 \bar{X} \\
\end{aligned}$$

1. Calculate the Bias What happens in the asymptote?

For $\theta_{MLE}$ we have:

Note, the sampling distribution to $\theta_{MLE}$  is given by:

$$\begin{aligned}
f_{\theta_{MLE}} \left( x \right) =\begin{Bmatrix} 
n \frac{x^{n-1}}{\theta^n} \ for\ 0 \leq x \leq \theta \\
0 \ \ otherwise
\end{Bmatrix}
\end{aligned}$$

Thus, we may evaluate $E_{\theta_{MLE}} (X) = \int \limits_{0}^{\theta} x f_{\theta_{MLE}} dx = \int \limits_{0}^{\theta} x n \frac{x^{n-1}}{\theta^n} dx = \frac{n x ^{n+1}}{(n+1)\theta^n} \Big|_0^\theta = \frac{n}{n+1} \theta$

Thus, we have:

$Bias_{\theta_{MLE}}= E_{\theta_{MLE}} \left( \hat\theta\right) - \theta = \frac{n}{n+1}\theta - \theta = - \frac{1}{n+1}\theta$

For $\theta_{MOM}$ we have:

$E_{\theta_{MOM}}(X) =  \int \limits_{0}^{\theta} x \frac{1}{\theta} dx = \frac{x^2}{2\theta} \Big|_0^\theta  = \frac{\theta}{2}$

$Bias_{\theta_{MOM}}= E_{\theta_{MOM}} \left( \hat\theta\right) - \theta = \frac{2 \theta} {2} - \theta = \theta - \theta = 0$

### Commentary

The above results indicate the MLE estimate is biased (but unbiased asymptotically as n increases) and the MOM estimate is unbiased.

### Note

The pdf of $X_{max}$ is $nF^{n - 1}(x)f(x).

2. Calculate the Variance What happens in the asymptote?

For $\theta_{MLE}$ we have:

$E_{\theta_{MLE}} (X^2) = \int \limits_{0}^{\theta} x^2 f_{\theta_{MLE}} dx = \int \limits_{0}^{\theta} x^2 n \frac{x^{n-1}}{\theta^n} dx = \frac{n x ^{n+2}}{(n+2)\theta^n} \Big|_0^\theta = \frac{n}{n+2} \theta^2$

$Var_{\theta_{MLE}}= E_{\theta_{MLE}} \left( {\hat\theta} ^ {2} \right) - \left(E_{\theta_{MLE}} \left( {\hat\theta}\right)\right) ^ {2}$

$=\frac{n}{n+2}\theta^2 - \left( \frac{n}{n+1} \theta \right)^2 = \frac{n}{ \left( n+1 \right) \left(n+2 \right)} \theta^2$

For $\theta_{MOM}$ we have:

$E_{\theta_{MOM}}(X^2) =  \int \limits_{0}^{\theta} x^2 \frac{1}{\theta} dx = \frac{x^3}{3\theta} \Big|_0^\theta  = \frac{\theta}{3}$

Thus, we have:

$Var_{\theta_{MOM}}= E_{\theta_{MOM}} \left( {\hat\theta} ^ {2} \right) - \left(E_{\theta_{MOM}} \left( {\hat\theta}\right)\right) ^ {2}$

$= \frac {\theta^2}{3} - \left( \frac{\theta}{2}\right)^2 = \frac{\theta^2}{3} - \frac{\theta^2}{4} = \frac{\theta^2}{12}$

Thus:

$V_\theta \left( \hat{\theta}_n \right) = \frac{4}{n^2} n V_\theta \left( X\right) = \frac{4}{n^2} n \frac{\theta^2}{12} = \frac{\theta^2} {3n}$

### Commentary

Asymptotically, both variances converge to 0 (as n increases).

3. Simulate and do ggridges plot to show convergence/consistency

``` {r, echo=TRUE, eval=TRUE}
library(ggridges)
set.seed(42)

# Calculate and simulate means

mle_mean <- rep(NA, 5000)
mom_mean <- rep(NA, 5000)

for (i in 1:500) {
  x <- sample(1:100, 1, replace=T)
  mle_mean[i] <- max(x)
  mom_mean[i] <- 2*mean(x)
}

# Calculate Variance

mle_var <- rep(NA, 5000)
mom_var <- rep(NA, 5000)

x_count <- (1:5000)

for (i in 1:5000) {
  mle_var[i] <- ((as.numeric(mle_mean[i]))^2) * as.numeric(x_count[i]) / ((as.numeric(x_count[i]) + 1) * (as.numeric(x_count[i]) + 2))
  mom_var[i] <- (as.numeric(mom_mean[i])^2) / (as.numeric(x_count[i]) *3)
}

moms <- rep(NA, 5000)
mles <- rep(NA, 5000)

# Naming convention for grouping later

for (i in 1:5000) {
moms[i] <- "mom"
mles[i] <- "mle"
}

# Coerce into one dataframe for ggridges

mom_est <- data.frame(cbind(x_count, moms, mom_mean, mom_var))
mle_est <- data.frame(cbind(x_count, mles, mle_mean, mle_var))

mom_est <- mom_est %>% 
  rename(
    estimate_mean = mom_mean,
    estimate_var = mom_var,
    type = moms
    )

mle_est <- mle_est %>% 
  rename(
    estimate_mean = mle_mean,
    estimate_var = mle_var,
    type= mles
    )

total <- rbind(mle_est, mom_est)

ggplot(total, aes(x = estimate_mean, y = type)) + geom_density_ridges()

ggplot(total, aes(x = estimate_var, y = type)) + geom_density_ridges()


```