---
title: "MATH 392 Problem Set 7"
output: 
  pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", 
                      message = FALSE, warning = FALSE)
library(tidyverse)
library(knitr)
library(here)
```

## Exercises from the book

**11.1: #4**

**Q:** For $i=1, ..., n$, let $\hat{y}_i = \beta_o + \beta_{1} x_i$.

Show that $\hat{\beta_0}$ and $\hat{\beta_1}$, as given in Eq. (11.1.1), are unique values of $\beta_0$ and $\beta_1$ such that:

$\sum \limits_{i=1}^{n} \left( y_i - \hat{y}_i \right) = 0$

And: 

$\sum \limits_{i=1}^{n} x_i\left( y_i - \hat{y}_i \right) = 0$

**Note:**

Eq. (11.1.1): 

(1): $\hat{\beta}_1 = \frac{\sum\limits_{i=1}^{n} \left(y_i - \bar{y} \right) \left(x_i - \bar{x} \right)} {\sum\limits_{i=1}^{n} {\left(x_i - \bar{x} \right) ^ 2}}$

(2): $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$

Eq. (11.1.3): 

(3): $\frac{\partial{Q}}{\partial{\beta_0}} = -2\sum \limits_{i=1}^{n} \left( y_i - \beta_0 - \beta_1{x_i} \right)$

Eq. (11.1.4): 

(4): $\frac{\partial{Q}}{\partial{\beta_1}} = -2\sum \limits_{i=1}^{n} \left( y_i - \beta_0 - \beta_1{x_i} \right) x_i$

**A:**

Note: The values of $\beta_0$ and $\beta_1$ must satisfy relation (3) as above, such that $\frac{\partial{Q}}{\partial{\beta_0}} = 0$. 

Thus, noting that $\hat{y}_i = \beta_0 + \beta_1 x_i$, and taking out the constant, this is to say: 

$\frac{\partial{Q}}{\partial{\beta_0}} = 0 = \sum \limits_{i=1}^{n} \left( y_i - \hat{y}_i \right)$. 

However, the above relations must also satisfy relation (4) given above, leading us to a similar statement:

$\frac{\partial{Q}}{\partial{\beta_1}} = 0 = \sum \limits_{i=1}^{n} \left( y_i - \hat{y}_i \right)x_i$

Combining the above relations together, we obtain the following, as noted in Eq. (11.1.5):

(5): $\beta_0 n + \beta_1 \sum \limits_{i=1}^{n} {x_i} = \sum \limits_{i=1}^{n} {y_i}$

(6): $\beta_0 \sum \limits_{i=1}^{n} {x_i} + \beta_1 \sum \limits_{i=1}^{n} {x_i^2} = \sum \limits_{i=1}^{n} {x_i y_i}$

We then note some shorthand, namely:

(7): $\bar{x} = \frac{1}{n} \sum \limits_{i=1}^{n} {x_i}$

(8): $\bar{y} = \frac{1}{n} \sum \limits_{i=1}^{n} {y_i}$

Bearing in mind relations (7) and (8), we may then note relations (5) and (6) are the same as those given in (1) and (2) respectively, such that we conclude that $\hat{\beta_0}$ and $\hat{\beta_1}$, as given in Eq. (11.1.1), are unique values of $\beta_0$ and $\beta_1$. 

## Notational Note

In the exercises that follow, we use the following shorthand: 

(*): $ss_x = \sum \limits_{i=1}^{n} {(x_i - \bar{x} _n)^2}$

**11.2 #2**

**Q:** Show that $E \left( \hat{\beta}_1\right) = \beta_1$

**Note:** 

Eq. (11.2.7)

(1): $\hat{\beta}_1 = \frac{\sum \limits_{i=1}^{n} \left( x_i - \bar{x} \right) Y_i}{ss_x}$  

**A:**

Note: $E(Y_i) = \beta_0 + \beta_1 x_i$. Combining this with the above relation (1) gives us: 

$E \left( \hat{\beta}_1\right) = \frac{\sum \limits_{i=1}^{n} \left( x_i - \bar{x}_n \right) \left(\beta_0 + \beta_1 x_i \right)}{ss_x}$

Then, note: $\sum \limits_{i=1}^{n} {(x_i - \bar{x}_n)} = 0$. 

Furthermore, we note: $\sum \limits_{i=1}^{n} {(x_i - \bar{x}_n)} x_i = \sum \limits_{i=1}^{n}{x_i(x_i - \bar{x}_n)} - \bar{x}_n \sum \limits_{i=1}^{n} {(x_i - \bar{x}_n)} = \sum \limits_{i=1}^{n} {(x_i - \bar{x}_n)}^2 = ss_x$

Thus, we're left with: 

$E \left( \hat{\beta}_1\right) = \frac{ss_x \beta_1}{ss_x} = \beta_1$

**11.2 #3**

**Q:** Show that $E \left( \hat{\beta}_0\right) = \beta_0$

**A:**

Note the following relations: 

(1): $E(\bar{Y}_n) = \frac{1}{n} \sum \limits_{i=1}^{n} {E(Y_i)}$

(2): $E(\bar{Y}_n) = \frac{1}{n} \sum \limits_{i=1}^{n} {\beta_0 + \beta_1 x_i}$

(3): $E(\bar{Y}_n) = \beta_0 + \beta_1 \bar{x}_n$

Thus, as: 

$\bar{Y}_n = \beta_0 + \beta_1 \bar{x}_n \rightarrow \beta_0 = \bar{Y}_n - \beta_1 \bar{x}_n$

We may note: 

$E \left( \hat{\beta}_0\right) = E(\bar{Y}_n - \beta_1 \bar{x}_n)$

Taking advantage of linearity, we have: 

$E \left( \hat{\beta}_0\right) = E(\bar{Y}_n) - E(\beta_1 \bar{x}_n)$

Using relation (3) given above, and the results of the prior exercise, we have: 

$E \left( \hat{\beta}_0\right) = \beta_0 + \beta_1 \bar{x}_n - \bar \beta_1 \bar{x}_n = \beta_0$

**11.2 #4**

**Q:** Show that $Var \left( \hat{\beta}_0\right)$ is as given in Eq. (11.2.5). 

**Note:** 

Eq. (11.2.5) 

(1): $\sigma^2 \left( \frac{1}{n} + \frac{\bar{x}_n^2}{ss_x}\right)$

**A:**

Note the beginning of the prior exercise, specifically:

$\hat{\beta}_0 = \bar{Y}_n - \hat{\beta_1}\bar{x}_n$

Breaking out this equation gives us: 

$\hat{\beta}_0 = \frac{1}{n} \sum \limits_{i=1}^{n} Y_i - \hat{\beta}_1\bar{x}_n$

Using the equation for $\hat{\beta}_1$ gives us: 

$\hat{\beta}_0 = \frac{1}{n} \sum \limits_{i=1}^{n} Y_i - \bar{x}_n \frac{\sum \limits_{i=1}^{n} {(x_i - \bar{x}_n)Y_i}}{ss_x}$

We can simplify this equation as follows: 

**Double Check this part**

$\hat{\beta}_0 = \sum \limits_{i=1}^{n} {\left( \frac{1}{n} - \frac{\bar{x}_n}{ss_x} \left( x_i - \bar{x}_n \right) Y_i\right)}$

Using the above equation for variance, we have: 

$Var(\hat{\beta}_0) = Var(\sum \limits_{i=1}^{n} {\left( \frac{1}{n} - \frac{\bar{x}_n}{ss_x} \left( x_i - \bar{x}_n \right) Y_i\right)})$

We assume independence of the Y's, such that we may say: 

$Var(\hat{\beta}_0) =\sum \limits_{i=1}^{n} {\left( \frac{1}{n} - \frac{\bar{x}_n}{ss_x} \left( x_i - \bar{x}_n \right)\right)^2  Var \left( Y_i \right)}$

Since each Y has variance $\sigma^2$, we have: 

$Var(\hat{\beta}_0) = \sigma^2 \sum \limits_{i=1}^{n} {\left( \frac{1}{n} - \frac{\bar{x}_n}{ss_x} \left( x_i - \bar{x}_n \right)\right)^2}$

Expanding this equation gives us: 

$Var(\hat{\beta}_0) = \sigma^2 \sum \limits_{i=1}^{n} {\left( \frac{1}{n^2} - \frac{2\bar{x}_n}{nss_x}\left( x_i - \bar{x}_n\right) + \frac{\bar{x}_n^2}{ss_x^2}\left( x_i - \bar{x}_n \right)^2 \right)}$

Additionally, we note an alternative form of $\hat{\beta}_1$ as follows: 

**Check this part too**

$\hat{\beta}_1 = \frac{\sum \limits_{i=1}^{n} {x_iy_i - n \bar{x}_n \bar{y}_n}}{\sum \limits_{i=1}^{n} {x_i^2 - n \bar{x}_n^2}}$

Giving us: 

$Var(\hat{\beta}_0) = \sigma^2 \left( \frac{1}{n} - 0 + \frac{\bar{x}_n^2}{ss_x}\right)$

Thus, we may conclude: 

$Var \left( \hat{\beta}_0\right) = \sigma^2 \left( \frac{1}{n} + \frac{\bar{x}_n^2}{ss_x}\right)$, as given in Eq. (11.2.5). 

**11.2 #5**

**Q:** Show that $Cov \left( \hat{\beta}_0 , \hat{\beta}_1 \right)$ is as given in Eq. (11.2.6). *Hint:* Use the result in Exercise 8 in Sec. 4.6.  

**Notes:** 

Eq. (11.2.6):

(1): $Cov\left( \hat{\beta}_0 , \hat{\beta}_1 \right) = - \frac{\bar{x} \sigma^2}{ s^2_x} \equiv - \frac{\bar{x} \sigma^2}{ ss_x}$

The equivalence relation is to solidfy the notation being used in this problemset, as noted in relation (*), and is repeated due to the reference to an equation used in the book. 

Exercise 8, Sec. 4.6.: 

(2): For $X_1, ..., X_m$ and $Y_1, ..., Y_n$ random variables, let $i= 1, ..., m$ and $j=1, ..., n$. Suppose $Cov(X_i, Y_j)$ exists and let $a_1, ..., a_m$ and $b_1, ..., b_n$ be constants. Then the following holds: 

(3): $Cov(\sum \limits_{i=1}^{m} {a_iX_i} , \sum \limits_{i=1}^{n} {b_jY_j}) = \sum \limits_{i=1}^{m} \sum \limits_{i=1}^{n} a_ib_j Cov( X_i Y_j)$

**A:**

Note: 

$Var(\bar{Y}_n) =  Var(\hat{\beta}_0) + Var(\hat{\beta_1} \bar{x}_n) + Cov(\hat{\beta}_0 , \hat{\beta_1} \bar{x}_n)$

Taking out values of $\bar{x}_n$ gives us: 

$Var(\bar{Y}_n) = Var(\hat{\beta}_0) + \bar{x}_n^2 Var(\hat{\beta}_1) + 2 \bar{x}_nCov(\hat{\beta_0} , \hat{\beta_1})$

Isolating $Cov\left( \hat{\beta}_0 , \hat{\beta}_1 \right)$ gives us: 

$Cov\left( \hat{\beta}_0 , \hat{\beta}_1 \right) = \frac{1}{2\bar{x}_n}\left( Var(\bar{Y}_n) - Var(\hat{\beta}_0) - \bar{x}_n^2 Var(\hat{\beta}_1) \right)$

Note: The above relation holds for $\bar{x}_n \neq 0$.

Using the equations for the Variance of $\bar{Y}_n, \hat{\beta}_0,$ and $\hat{\beta}_1$ gives us: 

$Cov\left( \hat{\beta}_0 , \hat{\beta}_1 \right) = \frac{1}{2\bar{x}_n} \left( \frac{\sigma^2}{n} - \frac{\sum \limits_{i=1}^{n} {x_i^2}}{nss_x}\sigma^2 - \frac{\bar{x}_n^2}{ss_x} \sigma^2\right)$

Bringing out like terms gives us: 

$Cov\left( \hat{\beta}_0 , \hat{\beta}_1 \right) = \frac{\sigma^2}{2\bar{x}_n} \left( \frac{ss_x - \sum \limits_{i=1}^{n} {x_i^2} - n\bar{x}_n^2}{nss_x}\right)$

Simplifying this equation gives us: 

$Cov\left( \hat{\beta}_0 , \hat{\beta}_1 \right) = \frac{\sigma^2}{2\bar{x}_n} \left( \frac{-2n\bar{x}_n^2}{nss_x}\right) = -\frac{\bar{x}_n^2 \sigma^2}{ss_x}$

### Another Condition

Let us then determine $Cov\left( \hat{\beta}_0 , \hat{\beta}_1 \right)$ when $\bar{x}_n = 0$. 

Under this assumption, we have: 

$\hat{\beta}_0 = \bar{Y}_n$. 

Noting $\bar{Y}_n = \frac{1}{n} \sum \limits_{i=1}^{n} {Y_i}$, we have: 

$Cov\left( \hat{\beta}_0 , \hat{\beta}_1 \right) = Cov\left( \frac{1}{n} \sum \limits_{i=1}^{n} {Y_i},  \frac{1}{ss_x} \sum \limits_{j=1}^{n} {x_jY_j}\right)$

Taking advantage of relation (3) given above gives us: 

$Cov\left( \hat{\beta}_0 , \hat{\beta}_1 \right) =  \frac{1}{nss_x} \sum \limits_{i=1}^{n} \sum \limits_{j=1}^{n} {x_j Cov(Y_i, Y_j)}$

In addition to taking advantage of relation (3) from Exercise 8, Sec. 4.6., note the Y's are independent and each has variance $\sigma^2$. Importantly, this means that for $i=j$, $Cov(Y_i, Y_j) = \sigma^2$, and for $i \neq j$, $Cov(Y_i, Y_j) = 0$.

Thus, we are left with the following relation: 

$Cov\left( \hat{\beta}_0 , \hat{\beta}_1 \right) = \frac{1}{nss_x} \sum \limits_{i=1}^{n} \sum \limits_{j=1}^{n} {x_j Cov(Y_i, Y_j)} = \frac{\sigma^2}{nss_x} \sum \limits_{i=1}^{n} x_i= \frac{\bar{x}_n \sigma^2}{ss_x} = 0$

However, under the condition of interest, $\bar{x}_n = 0$, we have: 
$Cov\left( \hat{\beta}_0 , \hat{\beta}_1 \right) =  \frac{\sigma^2}{nss_x} \sum \limits_{i=1}^{n} x_i= \frac{\bar{x}_n \sigma^2}{ss_x} = 0 = -\frac{\bar{x}_n^2 \sigma^2}{ss_x}$, and it holds that $Cov \left( \hat{\beta}_0 , \hat{\beta}_1 \right)$ is as given in Eq. (11.2.6) $\forall \bar{x}_n$.

## Additional Exercises

1. Consider the following dataset:

```{r}
set.seed(32)
n <- 10
x <- rnorm(n)
y <- -1 + 1.3 * x + rnorm(n, .3)
df <- data.frame(x, y)
```

We can find the least squares regression line by running `lm()` (which uses the normal equations), and extract the coefficient estimates.

```{r}
m1 <- lm(y ~ x, data = df)
coef(m1)
```

A more general approach to finding the estimates that optimize a loss function is to use a numerical optimization technique. Here we use `optim()` to minimize the RSS. By default this function uses the [Nelder-Mead algorithm](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method)), but you can also toggle to another algorithm such as [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) or select an entirely different optimization function/package.

```{r}
RSS <- function(par, x, y) {
  beta_0 <- par[1]
  beta_1 <- par[2]
  sum((y - (beta_0 + beta_1 * x))^2)
}
opt <- optim(par = c(0, 0), fn = RSS, x = x, y = y)
```

The `par` argument is the set of values of the two parameters that you want to initialize the algorithm at. You can try several different values and see if the final estimates agree. The final estimates are found in the `opt` object.

```{r}
opt$par
```

Which agree very closely with the analytical solutions from the normal equations.

a. Using numerical optimization, find the estimates that minimize two additional loss functions: a) the absolute deviation in the `y` and b) the squared deviation in the x.

```{r}
# A Absolute Deviation in the y
abs_dif <- function(par, x, y) {
  beta_0 <- par[1]
  beta_1 <- par[2]
  sum(abs(y - (beta_0 + beta_1 * x)))
}
opt_abs <- optim(par = c(1, 1), fn = abs_dif, x = x, y = y)

opt_abs$par
```

```{r}
# B Squared Deviation in the X
ss_x <- function(par, x, y) {
  beta_0 <- par[1]
  beta_1 <- par[2]
  sum((x - (y/beta_1) + (beta_0/beta_1) )^2)
}
opt_ss_x <- optim(par = c(1, 1), fn = ss_x, x = x, y = y)

opt_ss_x$par
```

b. Plot all three lines on top of a scatterplot of the data. Add an `annotate()` layer or legend to make it clear which line is which.

```{r}
ggplot(data=df, aes(x=x, y=y)) +
  geom_point(size=2, shape=1) + 
  geom_abline(slope = 1.2495164, intercept = -0.4488259, colour="red") +
  geom_abline(slope = 1.2176767, intercept = -0.5811111, colour="blue") +
  geom_abline(slope = 1.7879482, intercept = -0.5679669, colour="green") +
  annotate("text", x = -.25, y = -.35, label = "Squared Deviation in the Y Line") +
  annotate("text", x = .5, y = -.35, label = "Absolute Deviation Line") +
  annotate("text", x = -.25, y = -1.5, label = "Squared Deviation in the X Line")
```

c. Create a second scatterplot that again shows the least squares regression line. Add to this plot pairs of lines that represent each of the following intervals:
    - A confidence interval on $\beta_1$.
    - A confidence interval on $E(Y | X = x)$.
    - A prediction interval on $[Y | X = x]$.

```{r}
# compute 95% confidence interval for \beta_1
confint(m1)
```

```{r}
# compute 95% confidence interval for E(Y | X = x)
conf_values <-predict(m1, newdata = df, interval = 'confidence')
conf_data <- data.frame(conf_values)

df_conf <- cbind(df, conf_data)
df_conf
```

```{r}
# compute 95% prediction interval on [Y | X = x]
pred_values <- predict(m1, newdata = df, interval = 'prediction')
pred_data <- data.frame(pred_values)

df_pred <- cbind(df, pred_data)
df_pred
```

```{r}
ggplot(data=df, aes(x=x, y=y)) +
  geom_point(size=2, shape=1) +
  geom_ribbon(data=df_conf,aes(ymin=lwr,ymax=upr),alpha=0.3) +
  geom_ribbon(data=df_pred,aes(ymin=lwr,ymax=upr),alpha=0.3) +
  geom_abline(slope = m1$coefficients[2], intercept = m1$coefficients[1], colour="red") +
  geom_abline(slope = confint(m1)[2,1], intercept = m1$coefficients[1], colour="blue") +
  geom_abline(slope = confint(m1)[2,2], intercept = m1$coefficients[1], colour="blue") +
  annotate("text", x = -.25, y = 0.25, label = "Prediction Interval of Y's") +
  annotate("text", x = .5, y = -.45, label = "Confidence Interval of Y's") +
  annotate("text", x = -0, y = -1.35, label = "Confidence Interval of Beta 1 Coef.")
```

2. [Ecological Fallacy](https://en.wikipedia.org/wiki/Ecological_fallacy) refers to a situation where one draws inferences on the individual level from data that was collected at the group level.

```{r}
# install.packages("resampledata")
library(resampledata)
data(corrExerciseB)
```

a. Create a scatter plot of all of the data, which each group plotted in a different color. Add in the group means for each.

```{r}
library(ggplot2)
library(plyr)

mean_table <- ddply(corrExerciseB, .(Z), summarize,  X_mean =mean(X), Y_mean =mean(Y))

a_x_mean <- mean_table$X_mean[1]
a_y_mean <- mean_table$Y_mean[1]
b_x_mean <- mean_table$X_mean[2]
b_y_mean <- mean_table$Y_mean[2]
c_x_mean <- mean_table$X_mean[3]
c_y_mean <- mean_table$Y_mean[3]
d_x_mean <- mean_table$X_mean[4]
d_y_mean <- mean_table$Y_mean[4]

ggplot(data=corrExerciseB, aes(x=X, y=Y, color=Z)) +
  geom_point(size=2, shape=1) +
  scale_color_manual(values=c("green", "red", "blue", "purple")) +
  geom_line(aes(x=a_x_mean), colour="green") +
  geom_line(aes(y=a_y_mean), colour="green") +
  geom_line(aes(x=b_x_mean), colour="red") +
  geom_line(aes(y=b_y_mean), colour="red") +
  geom_line(aes(x=c_x_mean), colour="blue") +
  geom_line(aes(y=c_y_mean), colour="blue") +
  geom_line(aes(x=d_x_mean), colour="purple") +
  geom_line(aes(y=d_y_mean), colour="purple") +
  geom_text(x=a_x_mean, y=a_y_mean+.66, label="Group A Mean", colour="black") +
  geom_text(x=b_x_mean, y=b_y_mean+.66, label="Group B Mean", colour="black") +
  geom_text(x=c_x_mean, y=c_y_mean+.66, label="Group C Mean", colour="black") +
  geom_text(x=d_x_mean, y=d_y_mean+.66, label="Group D Mean", colour="black") +
  guides(color=guide_legend(title="Group")) 
```

b. Compute two sample correlations: one for the group means, the other for all of the data. Under which conditions, stated informally, will the correlation at the group level exceed that at the individual level? Do you expect that this is the a more common or less common feature of aggregated data in the real world?

```{r}
library(plyr)

# Group Level Correlations

cor_func <- function(corrExerciseB)
{
return(data.frame(COR = cor(corrExerciseB$X, corrExerciseB$Y)))
}

ddply(corrExerciseB, .(Z), cor_func)

# Validate Group Level Correlations

group_a_cor <- corrExerciseB[which(corrExerciseB$Z=='A'),]
cor(group_a_cor$X, group_a_cor$Y)

group_b_cor <- corrExerciseB[which(corrExerciseB$Z=='B'),]
cor(group_b_cor$X, group_b_cor$Y)

group_c_cor <- corrExerciseB[which(corrExerciseB$Z=='C'),]
cor(group_c_cor$X, group_c_cor$Y)

group_d_cor <- corrExerciseB[which(corrExerciseB$Z=='D'),]
cor(group_d_cor$X, group_d_cor$Y)

# Total Data Correlation Using Means
cor(mean_table$X, mean_table$Y)
```

This appears to be an example of Simpson's paradox, also known as the Yule-Simpson effect, which *'occurs when the marginal association between two categorical variables is qualitatively different from the partial association between the same two variables after controlling for one or more other variables.'*

To directly answer the question at hand, and to provide personal perspective beyond regurgitating a definition---as relevant as it may be---the correlations at the group level may exceed those at an individual level when there are large differences between the number of observations available across the groups being considered, in addition to differences between observations within the same group. 

Though this is common, I would caution against interpreting this as 'Defcon 1' for statisticans, because there are a host of factors to consider. Before dampening the argument though, I will point out that improving data literacy (including knowledge of fallacies, paradoxes, and other phenomenon) would go a long way to mitigating risks, and improving data literacy in targetted efforts (or perhaps to say group-level educational efforts) would certainly improve things. 

Nonetheless, I would dampen the heart-grabbing, table-clearing reaction one may have had by noting an ecological fallacy ultimately causes harm when it isn't considered, or when a decision is made without vetting the potential for an ecological fallacy to have occured. Additional dimensions to this point may be noted, particularly the application and field of study for the data in question. 

That being said, this is one of many concepts and observations to consider--such as correlation vs. causation, extrapolation, robustness of results, and countably many others---when handling data. At the very least, it would do good to prioritize this on the list of things to know. I'd recommend putting it above the Borel–Kolmogorov Paradox and the Banach–Tarski Paradox.

c. In a setting such as this, what is the consequence for your data analysis of committing an ecological fallacy?

The consequence of committing an ecological fallacy is having inappropriate conclusions. 

To expand on this point: If one uses the total correlation of X on Y (positive) to infer what happens in a particular group's value of Y when X increases, one would be led to inappropriate conclusions. For one: Inferences of groups 'B' and 'D', whose correlation value was negative, would likely assume the incorrect relationship by signage. By contrast, an inference of groups 'A' and 'C' would also be incorrect, though due to magnitude instead of signage. 

Akin to the observation of 'statistical significance' and 'magnitude significance', committing an ecological fallacy makes for inappropriate conclusions, both in signage and magnitude. 
