---
title: "MATH 392 Problem Set 7"
output: 
  pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", 
                      message = FALSE, warning = FALSE)
library(tidyverse)
library(knitr)
library(here)
```

## Exercises from the book

**11.1: #4**

**Q:** For $i=1, ..., n$, let $\hat{y}_i = \beta_o + \beta_{1} x_i$.

Show that $\hat{\beta_0}$ and $\hat{\beta_1}$, as given in Eq. (11.1.1), are unique values of $\beta_0$ and $\beta_1$ such that:

$\sum \limits_{i=1}^{n} \left( y_i - \hat{y}_i \right) = 0$

And: 

$\sum \limits_{i=1}^{n} x_i\left( y_i - \hat{y}_i \right) = 0$

**Note:**

Eq. (11.1.1): 

(1): $\hat{\beta}_1 = \frac{\sum\limits_{i=1}^{n} \left(y_i - \bar{y} \right) \left(x_i - \bar{x} \right)} {\sum\limits_{i=1}^{n} {\left(x_i - \bar{x} \right) ^ 2}}$

(2): $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$

**A:**


**11.2 #2**

**Q:** Show that $E \left( \hat{\beta}_1\right) = \beta_1$

**A:**


**11.2 #3**

**Q:** Show that $E \left( \hat{\beta}_0\right) = \beta_0$

**A:**


**11.2 #4**

**Q:** Show that $Var \left( \hat{\beta}_0\right)$ is as given in Eq. (11.2.5). 

**Note:** 

Eq. (11.2.6):

(1): $f_n \left( \mathbf{y} | \mathbf{x}, \beta_o, \beta_1, \sigma^2 \right) = \frac{1}{\left( 2 \pi \sigma^2 \right) ^ \frac{n}{2}} e^{-\frac{1}{2 \sigma^2} \sum \limits_{i=1} ^{n} {\left( y_i - \beta_0 - \beta_1 x_i \right) ^2}}$

**A:**


**11.2 #5**

**Q:** Show that $Cov \left( \hat{\beta}_0 , \hat{\beta}_1 \right)$ is as given in Eq. (11.2.6). *Hint:* Use the result in Exercise 8 in Sec. 4.6.  

**Notes:** 

Eq. (11.2.6):

(1): $Cov\left( \hat{\beta}_0 , \hat{\beta}_1 \right) = - \frac{\bar{x} \sigma^2}{ s^2_x} \equiv - \frac{\bar{x} \sigma^2}{ ss_x}$

*The equivalence relation is to solidfy the notation being used in this problemset.* 

Exercise 8, Sec. 4.6.: 

(1): For $X_1, ..., X_m$ and $Y_1, ..., Y_n$ random variables, let $i= 1, ..., m$ and $j=1, ..., n$. Suppose $Cov(X_i, Y_j)$ exists and let $a_1, ..., a_m$ and $b_1, ..., b_n$ be constants. Then the following holds: 

(2): $Cov(\sum \limits_{i=1}^{m} {a_iX_i} , \sum \limits_{i=1}^{n} {b_jY_j}) = \sum \limits_{i=1}^{m} \sum \limits_{i=1}^{n} a_ib_j Cov( X_i Y_j)$

**A:**


## Additional Exercises

1. Consider the following dataset:

```{r}
set.seed(32)
n <- 10
x <- rnorm(n)
y <- -1 + 1.3 * x + rnorm(n, .3)
df <- data.frame(x, y)
```

We can find the least squares regression line by running `lm()` (which uses the normal equations), and extract the coefficient estimates.

```{r}
m1 <- lm(y ~ x, data = df)
coef(m1)
```

A more general approach to finding the estimates that optimize a loss function is to use a numerical optimization technique. Here we use `optim()` to minimize the RSS. By default this function uses the [Nelder-Mead algorithm](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method)), but you can also toggle to another algorithm such as [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) or select an entirely different optimization function/package.

```{r}
RSS <- function(par, x, y) {
  beta_0 <- par[1]
  beta_1 <- par[2]
  sum((y - (beta_0 + beta_1 * x))^2)
}
opt <- optim(par = c(0, 0), fn = RSS, x = x, y = y)
```

The `par` argument is the set of values of the two parameters that you want to initialize the algorithm at. You can try several different values and see if the final estimates agree. The final estimates are found in the `opt` object.

```{r}
opt$par
```

Which agree very closely with the analytical solutions from the normal equations.

a. Using numerical optimization, find the estimates that minimize two additional loss functions: a) the absolute deviation in the `y` and b) the squared deviation in the x.

b. Plot all three lines on top of a scatterplot of the data. Add an `annotate()` layer or legend to make it clear which line is which.

c. Create a second scatterplot that again shows the least squares regression line. Add to this plot pairs of lines that represent each of the following intervals:
    - A confidence interval on $\beta_1$.
    - A confidence interval on $E(Y | X = x)$.
    - A prediction interval on $[Y | X = x]$.
    
2. [Ecological Fallacy](https://en.wikipedia.org/wiki/Ecological_fallacy) refers to a situation where one draws inferences on the individual level from data that was collected at the group level.

```{r}
# install.packages("resampledata")
library(resampledata)
data(corrExerciseB)
```

a. Create a scatter plot of all of the data, which each group plotted in a different color. Add in the group means for each.

b. Compute two sample correlations: one for the group means, the other for all of the data. Under which conditions, stated informally, will the correlation at the group level exceed that at the individual level? Do you expect that this is the a more common or less common feature of aggregated data in the real world?

c. In a setting such as this, what is the consequence for your data analysis of committing an ecological fallacy?